{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12608684,"sourceType":"datasetVersion","datasetId":7964540}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:38:29.555846Z","iopub.execute_input":"2025-08-14T16:38:29.556183Z","iopub.status.idle":"2025-08-14T16:38:29.919751Z","shell.execute_reply.started":"2025-08-14T16:38:29.556124Z","shell.execute_reply":"2025-08-14T16:38:29.918875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/airlines-flights-data/airlines_flights_data.csv')\ndf1 = df.copy()\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:38:29.921286Z","iopub.execute_input":"2025-08-14T16:38:29.922060Z","iopub.status.idle":"2025-08-14T16:38:30.775735Z","shell.execute_reply.started":"2025-08-14T16:38:29.922034Z","shell.execute_reply":"2025-08-14T16:38:30.774704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ydata-profiling\n!pip install pydantic-settings\nfrom pydantic_settings import BaseSettings\nfrom ydata_profiling import ProfileReport\n\ndef profile_data(df: pd.DataFrame, output_file: str = None):\n    \"\"\"\n    Generate an interactive data profiling report.\n    \"\"\"\n    profile = ProfileReport(df, title=\"Dataset Profiling Report\", explorative=True)\n    if output_file:\n        profile.to_file(output_file)\n    else:\n        profile.to_notebook_iframe()\n    return profile\n\ndef auto_detect_problem_type(df: pd.DataFrame, target_col: str = None) -> str:\n    \"\"\"\n    Automatically detect the ML problem type based on target column and dataset.\n    \"\"\"\n    if target_col is None or target_col not in df.columns:\n        return \"unsupervised (clustering or other)\"\n\n    target = df[target_col]\n    datetime_cols = df.select_dtypes(include=['datetime64[ns]', 'datetime64']).columns\n    if pd.api.types.is_datetime64_any_dtype(df.index) or len(datetime_cols) > 0:\n        if pd.api.types.is_numeric_dtype(target):\n            return \"time series regression\"\n        else:\n            return \"time series classification\"\n\n    if pd.api.types.is_numeric_dtype(target):\n        unique_vals = target.nunique()\n        if unique_vals > 20:\n            return \"regression\"\n        else:\n            return \"classification\"\n\n    if pd.api.types.is_categorical_dtype(target) or target.dtype == object:\n        return \"classification\"\n\n    return \"unknown\"\n\n# Example usage\n# DATA_PATH = \"/kaggle/input/airlines-flights-data/airlines_flights_data.csv\"\nTARGET_COL = \"duration\"  # Replace with actual target column name\n\n# df = pd.read_csv(DATA_PATH)\n\n# Profile dataset (generates interactive report in notebook or saves to file)\nprofile = profile_data(df)  # For Jupyter notebook display\n# profile = profile_data(df, output_file=\"data_profile_report.html\")  # To save HTML report\n\n# Auto detect problem type\nproblem_type = auto_detect_problem_type(df, TARGET_COL)\nprint(f\"Auto-detected problem type: {problem_type}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:38:30.776690Z","iopub.execute_input":"2025-08-14T16:38:30.776997Z","iopub.status.idle":"2025-08-14T16:39:07.979346Z","shell.execute_reply.started":"2025-08-14T16:38:30.776969Z","shell.execute_reply":"2025-08-14T16:39:07.978056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# Separate numerical and categorical columns\n# Separate numerical and categorical columns\n# num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n# cat_cols = df.select_dtypes(include=['object', 'category']).columns\n\n# df1 = df.copy()\n# Separate numerical and categorical columns\nnum_cols = df1.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df1.select_dtypes(include=['object', 'category']).columns\n\ndef remove_duplicates(df1):\n    \"\"\"\n    Remove duplicate rows from dataset.\n    \"\"\"\n    before = len(df1)\n    df1 = df1.drop_duplicates()\n    after = len(df1)\n    print(f\"Duplicates removed: {before - after}\")\n    print(\"remove dups completed\")\n    return df1\nremove_duplicates(df1)\n\n\ndef check_missing_values(df1):\n    \"\"\"\n    Display missing value counts and percentages per column.\n    \"\"\"\n    missing = df1.isnull().sum()\n    percent = (missing / len(df1)) * 100\n    missing_df = pd.DataFrame({'MissingCount': missing, 'MissingPercent': percent})\n    print(missing_df[missing_df['MissingCount'] > 0])\ncheck_missing_values(df1)\n\n# Imputation example: mean for numerical, mode for categorical\ndef impute_missing_values(df1, num_cols, cat_cols):\n    \"\"\"\n    Impute missing values in numerical and categorical columns.\n    \"\"\"\n    for col in num_cols:\n        mean_value = df1[col].mean()\n        df1[col].fillna(mean_value, inplace=True)\n\n    for col in cat_cols:\n        mode_value = df1[col].mode()[0]\n        df1[col].fillna(mode_value, inplace=True)\n    return df1\nimpute_missing_values(df1, num_cols,cat_cols )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:07.981987Z","iopub.execute_input":"2025-08-14T16:39:07.982596Z","iopub.status.idle":"2025-08-14T16:39:08.663886Z","shell.execute_reply.started":"2025-08-14T16:39:07.982562Z","shell.execute_reply":"2025-08-14T16:39:08.663049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:08.664729Z","iopub.execute_input":"2025-08-14T16:39:08.664992Z","iopub.status.idle":"2025-08-14T16:39:08.793281Z","shell.execute_reply.started":"2025-08-14T16:39:08.664969Z","shell.execute_reply":"2025-08-14T16:39:08.792458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:08.794324Z","iopub.execute_input":"2025-08-14T16:39:08.794755Z","iopub.status.idle":"2025-08-14T16:39:08.926168Z","shell.execute_reply.started":"2025-08-14T16:39:08.794727Z","shell.execute_reply":"2025-08-14T16:39:08.925378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# # Load dataset (replace with actual path)\n# DATA_PATH = \"data/dataset.csv\"\n# df = pd.read_csv(DATA_PATH)\n\n# df = df.drop(columns = {\"df_index\"})\n\n# Basic statistical summary\nprint(df1.describe(include='all'))\n\n# Separate numerical and categorical columns\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df.select_dtypes(include=['object', 'category']).columns\n\n# Histograms for numerical features\ndf[num_cols].hist(bins=20, figsize=(15, 10))\nplt.suptitle(\"Numerical Feature Distributions\")\nplt.show()\n\n# Boxplots for outlier visualization\nfor col in num_cols:\n    plt.figure(figsize=(6, 3))\n    sns.boxplot(x=df[col])\n    plt.title(f\"Boxplot of {col}\")\n    plt.show()\n\n# Correlation heatmap (Pearson)\nplt.figure(figsize=(12, 8))\nsns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n# Countplots for categorical features\nfor col in cat_cols:\n    plt.figure(figsize=(8, 4))\n    sns.countplot(data=df, x=col, order=df[col].value_counts().index)\n    plt.title(f\"Category Counts for {col}\")\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Missing values heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(df1.isnull(), cbar=False, yticklabels=False, cmap='viridis')\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:08.927320Z","iopub.execute_input":"2025-08-14T16:39:08.928127Z","iopub.status.idle":"2025-08-14T16:39:26.740907Z","shell.execute_reply.started":"2025-08-14T16:39:08.928083Z","shell.execute_reply":"2025-08-14T16:39:26.740134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import shapiro\n\n# Assuming 'df' is your DataFrame, already loaded.\n# For example: df = pd.read_csv('your_data.csv')\n\n# List of columns to check for normality\ncolumns_to_check = ['duration', 'price']\n\n# Create a figure for plotting\nfig, axes = plt.subplots(1, len(columns_to_check), figsize=(12, 5))\nfig.suptitle('Distribution of Key Columns', fontsize=16)\n\nfor i, col in enumerate(columns_to_check):\n    print(f\"\\n--- Checking Normality for '{col}' ---\")\n    \n    # Drop missing values for the statistical test\n    data_to_test = df1[col].dropna()\n    \n    # Visual check with a histogram\n    sns.histplot(data_to_test, kde=True, ax=axes[i])\n    axes[i].set_title(f'Histogram of {col}')\n    \n    # Statistical check with Shapiro-Wilk test\n    stat, p_value = shapiro(data_to_test)\n    \n    print(f\"Shapiro-Wilk Test Statistic: {stat:.4f}\")\n    print(f\"P-value: {p_value:.4f}\")\n    \n    # Interpretation\n    if p_value > 0.05:\n        print(f\"Conclusion: The '{col}' column likely comes from a normal distribution.\")\n    else:\n        print(f\"Conclusion: The '{col}' column likely does not come from a normal distribution.\")\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:26.741705Z","iopub.execute_input":"2025-08-14T16:39:26.742048Z","iopub.status.idle":"2025-08-14T16:39:30.329654Z","shell.execute_reply.started":"2025-08-14T16:39:26.742010Z","shell.execute_reply":"2025-08-14T16:39:30.328684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndef plot_boxplots(data):\n    \"\"\"\n    Plot boxplots for all numeric columns in a DataFrame.\n    \"\"\"\n    # Find all numeric columns in the DataFrame\n    numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n\n    # Check if any numeric columns were found\n    if not numeric_cols:\n        print(\"No numeric columns found to plot.\")\n        return\n\n    # Loop through the list of numeric column names\n    for col in numeric_cols:\n        plt.figure(figsize=(8, 4)) # Adjusted figure size for better readability\n        sns.boxplot(x=data[col])\n        plt.title(f\"Boxplot of {col}\")\n        plt.show()\n\nplot_boxplots(df1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:30.330922Z","iopub.execute_input":"2025-08-14T16:39:30.331331Z","iopub.status.idle":"2025-08-14T16:39:30.810087Z","shell.execute_reply.started":"2025-08-14T16:39:30.331300Z","shell.execute_reply":"2025-08-14T16:39:30.809234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop('df_index', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:30.813002Z","iopub.execute_input":"2025-08-14T16:39:30.813283Z","iopub.status.idle":"2025-08-14T16:39:30.834226Z","shell.execute_reply.started":"2025-08-14T16:39:30.813260Z","shell.execute_reply":"2025-08-14T16:39:30.833173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#After Cleaning as well if any outliers use winsorize to cap. outliers correction only applicatble for num cols. \n#if col hav outlier is normal then apply zscore technique\n#when not col data is not normal or skew then use below normal to find the outliers\n\n\n# def detect_outliers_iqr(data, col):\n#     \"\"\"\n#     Detect outliers in a column using the IQR method.\n#     \"\"\"\n#     Q1 = data[col].quantile(0.25)\n#     Q3 = data[col].quantile(0.75)\n#     IQR = Q3 - Q1\n#     lower_bound = Q1 - 1.5 * IQR\n#     upper_bound = Q3 + 1.5 * IQR\n#     outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n#     return outliers\n\n# outliers_in_duration = detect_outliers_iqr(df1, 'duration')\n# outliers_in_price = detect_outliers_iqr(df1, 'price')\n\n# # Now you can print or view the results.\n# print(\"Outliers in the 'duration' column:\")\n# print(outliers_in_duration)\n\n# print(\"\\nOutliers in the 'price' column:\")\n# print(outliers_in_price)\n\nfrom scipy.stats import zscore\ndef detect_outliers_zscore(data, col, threshold=3):\n    \"\"\"\n    Detect outliers in a column using the Z-score method.\n    \"\"\"\n    # Create a Series with Z-scores and align its index with the original data\n    z_scores_series = data[col].dropna()\n    z_scores = zscore(z_scores_series)\n\n    # Use the index of the Z-scores to filter the original DataFrame\n    outlier_indices = z_scores_series[np.abs(z_scores) > threshold].index\n    outliers = data.loc[outlier_indices]\n    \n    return outliers\n\ndetect_outliers_zscore(df, 'duration',threshold=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:30.835266Z","iopub.execute_input":"2025-08-14T16:39:30.835616Z","iopub.status.idle":"2025-08-14T16:39:30.868375Z","shell.execute_reply.started":"2025-08-14T16:39:30.835592Z","shell.execute_reply":"2025-08-14T16:39:30.867360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# No outliers skipping winsorization to cap\n\n\n\nnumeric_cols = df1.select_dtypes(include=np.number).columns.tolist()\ndef winsorize_column(data, col):\n    \"\"\"\n    Cap outliers using the IQR method bounds.\n    \"\"\"\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])\n    data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])\n    return df\nwinsorize_column(df1,numeric_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:30.869378Z","iopub.execute_input":"2025-08-14T16:39:30.869702Z","iopub.status.idle":"2025-08-14T16:39:30.966284Z","shell.execute_reply.started":"2025-08-14T16:39:30.869672Z","shell.execute_reply":"2025-08-14T16:39:30.964877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Same here not outliers..skipping winsorization\n\nfrom scipy.stats.mstats import winsorize\nnum_cols = df1.select_dtypes(include=['int64', 'float64']).columns\n\n# Detect outliers for first numerical column using IQR\ncolumn_to_check = num_cols[0]\noutliers_iqr = detect_outliers_zscore(df1, column_to_check)\nprint(f\"Outliers detected by IQR in {column_to_check}:\")\nprint(outliers_iqr)\n\n# Visualize outliers\nplot_boxplots(df1)\n\n# Treat outliers with winsorization\ndf = winsorize_column(df1, column_to_check)\nprint(f\"After winsorization, summary stats for {column_to_check}:\")\nprint(df1[column_to_check].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:30.967339Z","iopub.execute_input":"2025-08-14T16:39:30.967680Z","iopub.status.idle":"2025-08-14T16:39:31.533027Z","shell.execute_reply.started":"2025-08-14T16:39:30.967648Z","shell.execute_reply":"2025-08-14T16:39:31.532065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.534135Z","iopub.execute_input":"2025-08-14T16:39:31.534411Z","iopub.status.idle":"2025-08-14T16:39:31.540324Z","shell.execute_reply.started":"2025-08-14T16:39:31.534368Z","shell.execute_reply":"2025-08-14T16:39:31.539316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.541474Z","iopub.execute_input":"2025-08-14T16:39:31.541799Z","iopub.status.idle":"2025-08-14T16:39:31.561184Z","shell.execute_reply.started":"2025-08-14T16:39:31.541771Z","shell.execute_reply":"2025-08-14T16:39:31.560355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.562383Z","iopub.execute_input":"2025-08-14T16:39:31.562741Z","iopub.status.idle":"2025-08-14T16:39:31.593275Z","shell.execute_reply.started":"2025-08-14T16:39:31.562711Z","shell.execute_reply":"2025-08-14T16:39:31.592140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.594565Z","iopub.execute_input":"2025-08-14T16:39:31.595543Z","iopub.status.idle":"2025-08-14T16:39:31.615634Z","shell.execute_reply.started":"2025-08-14T16:39:31.595516Z","shell.execute_reply":"2025-08-14T16:39:31.613685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.sample(n=10, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.617238Z","iopub.execute_input":"2025-08-14T16:39:31.617588Z","iopub.status.idle":"2025-08-14T16:39:31.657261Z","shell.execute_reply.started":"2025-08-14T16:39:31.617552Z","shell.execute_reply":"2025-08-14T16:39:31.656374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.preprocessing import LabelEncoder\n\n# def heuristic_auto_encode(df, ordinal_threshold=5):\n#     \"\"\"\n#     Automatically applies Label Encoding or One-Hot Encoding based on a\n#     cardinality threshold.\n    \n#     Args:\n#         df (pd.DataFrame): The DataFrame to encode.\n#         ordinal_threshold (int): The maximum number of unique categories\n#                                  for a column to be considered for Label Encoding.\n        \n#     Returns:\n#         pd.DataFrame: A new DataFrame with all categorical columns encoded.\n#     \"\"\"\n#     encoded_df = df.copy()\n    \n#     all_cat_cols = encoded_df.select_dtypes(include=['object', 'category']).columns.tolist()\n    \n#     label_encode_cols = []\n#     ohe_cols = []\n    \n#     for col in all_cat_cols:\n#         if encoded_df[col].nunique() <= ordinal_threshold:\n#             # Assume columns with few unique values are ordinal\n#             label_encode_cols.append(col)\n#         else:\n#             # Assume columns with many unique values are nominal\n#             ohe_cols.append(col)\n            \n#     # Apply Label Encoding\n#     for col in label_encode_cols:\n#         le = LabelEncoder()\n#         encoded_df[col] = le.fit_transform(encoded_df[col])\n#         print(f\"Applied Label Encoding to: {col} (Unique values: {df[col].nunique()})\")\n    \n#     # Apply One-Hot Encoding\n#     encoded_df = pd.get_dummies(encoded_df, columns=ohe_cols, drop_first=True)\n#     print(f\"Applied One-Hot Encoding to: {ohe_cols}\")\n        \n#     return encoded_df\n\n# # --- How to use this function ---\n# # Assuming df1 is your cleaned DataFrame\n# df_encoded = heuristic_auto_encode(df1, ordinal_threshold=5)\n\n# print(\"\\nFirst 5 rows of the encoded DataFrame:\")\n# print(df_encoded.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.658286Z","iopub.execute_input":"2025-08-14T16:39:31.659272Z","iopub.status.idle":"2025-08-14T16:39:31.663975Z","shell.execute_reply.started":"2025-08-14T16:39:31.659240Z","shell.execute_reply":"2025-08-14T16:39:31.663105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the count of unique values in the 'flight' column\nunique_flight_count = df1['flight'].nunique()\n\n# Print the result\nprint(f\"The 'flight' column has {unique_flight_count} unique values.\")\n\n# You can also check the total number of rows in the DataFrame for comparison\ntotal_rows = len(df1)\nprint(f\"The entire DataFrame has {total_rows} rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.664983Z","iopub.execute_input":"2025-08-14T16:39:31.665649Z","iopub.status.idle":"2025-08-14T16:39:31.708726Z","shell.execute_reply.started":"2025-08-14T16:39:31.665616Z","shell.execute_reply":"2025-08-14T16:39:31.707757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if unique values are 95% in a clumn then we should not apply encoders to it\n# Assuming your DataFrame is named 'df'\ntotal_rows = len(df1)\n\n# Check all columns and identify those with high cardinality\ncols_to_exclude = []\nfor col in df1.columns:\n    unique_ratio = df1[col].nunique() / total_rows\n    if unique_ratio > 0.95:  # You can adjust this threshold\n        cols_to_exclude.append(col)\n        print(f\"Column '{col}' has a unique ratio of {unique_ratio:.2f} and should likely be excluded.\")\n\nprint(f\"\\nFinal list of columns to exclude: {cols_to_exclude}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.709670Z","iopub.execute_input":"2025-08-14T16:39:31.709951Z","iopub.status.idle":"2025-08-14T16:39:31.877226Z","shell.execute_reply.started":"2025-08-14T16:39:31.709923Z","shell.execute_reply":"2025-08-14T16:39:31.876370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport category_encoders as ce\n\ndef magic_auto_encode(df, low_cardinality_threshold=5, high_cardinality_threshold=50):\n    \"\"\"\n    Automatically applies different encoders based on the number of unique values.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to encode.\n        low_cardinality_threshold (int): Max unique values for Label Encoding.\n        high_cardinality_threshold (int): Min unique values for Binary Encoding.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with all categorical columns encoded.\n    \"\"\"\n    encoded_df = df1.copy()\n    \n    # Identify all categorical columns\n    all_cat_cols = encoded_df.select_dtypes(include=['object', 'category']).columns.tolist()\n    label_encode_cols = []\n    ohe_cols = []\n    binary_encode_cols = []\n    \n    for col in all_cat_cols:\n        n_unique = encoded_df[col].nunique()\n        if n_unique <= low_cardinality_threshold:\n            label_encode_cols.append(col)\n        elif n_unique > high_cardinality_threshold:\n            binary_encode_cols.append(col)\n        else:\n            ohe_cols.append(col)\n    \n    print(\"--- Encoding Summary ---\")\n    print(f\"Applying Label Encoding to: {label_encode_cols}\")\n    print(f\"Applying One-Hot Encoding to: {ohe_cols}\")\n    print(f\"Applying Binary Encoding to: {binary_encode_cols}\")\n    \n    # Apply Label Encoding\n    for col in label_encode_cols:\n        le = LabelEncoder()\n        encoded_df[col] = le.fit_transform(encoded_df[col])\n    \n    # Apply One-Hot Encoding\n    encoded_df = pd.get_dummies(encoded_df, columns=ohe_cols, drop_first=True)\n    \n    # Apply Binary Encoding\n    if binary_encode_cols:\n        encoder = ce.BinaryEncoder(cols=binary_encode_cols)\n        encoded_df = encoder.fit_transform(encoded_df)\n        \n    return encoded_df\n\n# --- How to use this function ---\n# Assuming df1 is your cleaned DataFrame\ndf_encoded = magic_auto_encode(df1, low_cardinality_threshold=5, high_cardinality_threshold=50)\n\nprint(\"\\nFirst 5 rows of the encoded DataFrame:\")\nprint(df_encoded.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:31.878373Z","iopub.execute_input":"2025-08-14T16:39:31.878689Z","iopub.status.idle":"2025-08-14T16:39:33.375621Z","shell.execute_reply.started":"2025-08-14T16:39:31.878647Z","shell.execute_reply":"2025-08-14T16:39:33.374787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# import category_encoders as ce\n\n# def magic_auto_encode(df, low_cardinality_threshold=5, high_cardinality_threshold=50, high_cardinality_strategy='binary', exclude_cols=[]):\n#     \"\"\"\n#     Automatically applies different encoders based on cardinality, with\n#     an option for high-cardinality columns.\n\n#     Args:\n#         df (pd.DataFrame): The DataFrame to encode.\n#         low_cardinality_threshold (int): Max unique values for Label Encoding.\n#         high_cardinality_threshold (int): Min unique values for high-cardinality strategy.\n#         high_cardinality_strategy (str): 'binary' or 'frequency'.\n#         exclude_cols (list): A list of columns to exclude from encoding.\n\n#     Returns:\n#         pd.DataFrame: A new DataFrame with all categorical columns encoded.\n#     \"\"\"\n#     encoded_df = df.copy()\n    \n#     all_cat_cols = [col for col in encoded_df.select_dtypes(include=['object', 'category']).columns.tolist() if col not in magic_auto_encode]\n    \n#     label_encode_cols = []\n#     ohe_cols = []\n#     high_card_cols = []\n    \n#     for col in all_cat_cols:\n#         n_unique = encoded_df[col].nunique()\n#         if n_unique <= low_cardinality_threshold:\n#             label_encode_cols.append(col)\n#         elif n_unique > high_cardinality_threshold:\n#             high_card_cols.append(col)\n#         else:\n#             ohe_cols.append(col)\n    \n#     print(\"--- Encoding Summary ---\")\n#     print(f\"Applying Label Encoding to: {label_encode_cols}\")\n#     print(f\"Applying One-Hot Encoding to: {ohe_cols}\")\n#     print(f\"Applying {high_cardinality_strategy} Encoding to: {high_card_cols}\")\n\n#     # Apply Label Encoding\n#     for col in label_encode_cols:\n#         le = LabelEncoder()\n#         encoded_df[col] = le.fit_transform(encoded_df[col])\n    \n#     # Apply One-Hot Encoding\n#     encoded_df = pd.get_dummies(encoded_df, columns=ohe_cols, drop_first=True)\n    \n#     # Apply high-cardinality strategy\n#     if high_card_cols:\n#         if high_cardinality_strategy == 'binary':\n#             encoder = ce.BinaryEncoder(cols=high_card_cols)\n#             encoded_df = encoder.fit_transform(encoded_df)\n#         elif high_cardinality_strategy == 'frequency':\n#             for col in high_card_cols:\n#                 freq_map = encoded_df[col].value_counts(normalize=True)\n#                 encoded_df[f'{col}_freq'] = encoded_df[col].map(freq_map)\n#                 encoded_df = encoded_df.drop(columns=[col])\n        \n#     return encoded_df\n\n# # --- How to use this function ---\n# # Example 1: Use Binary Encoding for high-cardinality columns\n# df_encoded_binary = magic_auto_encode(df1, low_cardinality_threshold=5, high_cardinality_threshold=50, high_cardinality_strategy='binary', exclude_cols=['flight'])\n\n# # Example 2: Use Frequency Encoding for high-cardinality columns\n# df_encoded_freq = magic_auto_encode(df1, low_cardinality_threshold=5, high_cardinality_threshold=50, high_cardinality_strategy='frequency', exclude_cols=['flight'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.376554Z","iopub.execute_input":"2025-08-14T16:39:33.377045Z","iopub.status.idle":"2025-08-14T16:39:33.382957Z","shell.execute_reply.started":"2025-08-14T16:39:33.377023Z","shell.execute_reply":"2025-08-14T16:39:33.381912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# import category_encoders as ce\n\n# def magic_auto_encode_with_target(df, target_col, low_cardinality_threshold=5, high_cardinality_threshold=50, high_cardinality_strategy='binary', exclude_cols=[]):\n#     \"\"\"\n#     Automatically applies different encoders based on cardinality, including\n#     Target Encoding for high-cardinality columns.\n\n#     Args:\n#         df (pd.DataFrame): The DataFrame to encode.\n#         target_col (str): The name of the target column.\n#         low_cardinality_threshold (int): Max unique values for Label Encoding.\n#         high_cardinality_threshold (int): Min unique values for high-cardinality strategy.\n#         high_cardinality_strategy (str): 'binary', 'frequency', or 'target'.\n#         exclude_cols (list): A list of columns to exclude from encoding.\n\n#     Returns:\n#         pd.DataFrame: A new DataFrame with all categorical columns encoded.\n#     \"\"\"\n#     # Check if the target column exists in the DataFrame\n#     if high_cardinality_strategy == 'target' and target_col not in df.columns:\n#         raise ValueError(f\"Target column '{target_col}' not found in DataFrame.\")\n\n#     encoded_df = df.copy()\n    \n#     # Exclude the target column and manually excluded columns from encoding\n#     excluded_from_encoding = exclude_cols + [target_col]\n#     all_cat_cols = [col for col in encoded_df.select_dtypes(include=['object', 'category']).columns.tolist() \n#                     if col not in excluded_from_encoding]\n    \n#     label_encode_cols = []\n#     ohe_cols = []\n#     high_card_cols = []\n    \n#     for col in all_cat_cols:\n#         n_unique = encoded_df[col].nunique()\n#         if n_unique <= low_cardinality_threshold:\n#             label_encode_cols.append(col)\n#         elif n_unique > high_cardinality_threshold:\n#             high_card_cols.append(col)\n#         else:\n#             ohe_cols.append(col)\n    \n#     print(\"--- Encoding Summary ---\")\n#     print(f\"Applying Label Encoding to: {label_encode_cols}\")\n#     print(f\"Applying One-Hot Encoding to: {ohe_cols}\")\n#     print(f\"Applying {high_cardinality_strategy} Encoding to: {high_card_cols}\")\n\n#     # Apply Label Encoding\n#     for col in label_encode_cols:\n#         le = LabelEncoder()\n#         encoded_df[col] = le.fit_transform(encoded_df[col])\n    \n#     # Apply One-Hot Encoding\n#     encoded_df = pd.get_dummies(encoded_df, columns=ohe_cols, drop_first=True)\n    \n#     # Apply high-cardinality strategy\n#     if high_card_cols:\n#         if high_cardinality_strategy == 'binary':\n#             encoder = ce.BinaryEncoder(cols=high_card_cols)\n#             encoded_df = encoder.fit_transform(encoded_df)\n#         elif high_cardinality_strategy == 'frequency':\n#             for col in high_card_cols:\n#                 freq_map = encoded_df[col].value_counts(normalize=True)\n#                 encoded_df[f'{col}_freq'] = encoded_df[col].map(freq_map)\n#                 encoded_df = encoded_df.drop(columns=[col])\n#         elif high_cardinality_strategy == 'target':\n#             # Target Encoding using a cross-validation approach to prevent data leakage\n#             encoder = ce.TargetEncoder(cols=high_card_cols)\n#             encoded_df = encoder.fit_transform(encoded_df, encoded_df[target_col])\n            \n#     return encoded_df\n\n# # --- HOW TO USE WITH YOUR DATAFRAME ---\n# # Assuming your DataFrame is named 'df' and your target column is 'target_label'\n# # Replace 'target_label' with the actual name of your target column.\n# # Replace the list ['col1_to_exclude', 'col2_to_exclude'] with your actual columns to exclude.\n\n# # Example 1: Use Target Encoding for high-cardinality columns\n# my_encoded_df = magic_auto_encode_with_target(\n#     df=df,\n#     target_col='target_label',  # <-- REPLACE WITH YOUR TARGET COLUMN NAME\n#     low_cardinality_threshold=10, \n#     high_cardinality_threshold=100, \n#     high_cardinality_strategy='target',\n#     exclude_cols=['col1_to_exclude', 'col2_to_exclude'] # <-- REPLACE WITH YOUR COLUMNS TO EXCLUDE\n# )\n\n# print(my_encoded_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.383975Z","iopub.execute_input":"2025-08-14T16:39:33.384275Z","iopub.status.idle":"2025-08-14T16:39:33.411084Z","shell.execute_reply.started":"2025-08-14T16:39:33.384253Z","shell.execute_reply":"2025-08-14T16:39:33.410103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_encoded.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.412229Z","iopub.execute_input":"2025-08-14T16:39:33.412534Z","iopub.status.idle":"2025-08-14T16:39:33.449240Z","shell.execute_reply.started":"2025-08-14T16:39:33.412511Z","shell.execute_reply":"2025-08-14T16:39:33.448248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.preprocessing import LabelEncoder\n\n# def auto_encode_df(df, ordinal_cols):\n#     \"\"\"\n#     Automatically applies Label Encoding to ordinal columns and One-Hot Encoding\n#     to all other categorical columns.\n    \n#     Args:\n#         df (pd.DataFrame): The DataFrame to encode.\n#         ordinal_cols (list): A list of column names to be treated as ordinal.\n        \n#     Returns:\n#         pd.DataFrame: A new DataFrame with all categorical columns encoded.\n#     \"\"\"\n#     encoded_df = df.copy()\n    \n#     # Identify all categorical columns\n#     all_cat_cols = encoded_df.select_dtypes(include=['object', 'category']).columns.tolist()\n    \n#     # Identify nominal columns by excluding the ordinal ones\n#     nominal_cols = [col for col in all_cat_cols if col not in ordinal_cols]\n    \n#     # Apply One-Hot Encoding to nominal columns\n#     encoded_df = pd.get_dummies(encoded_df, columns=nominal_cols, drop_first=True)\n    \n#     # Apply Label Encoding to ordinal columns\n#     for col in ordinal_cols:\n#         le = LabelEncoder()\n#         encoded_df[col] = le.fit_transform(encoded_df[col])\n        \n#     return encoded_df\n\n# # --- How to use this function ---\n# # Assuming df1 is your cleaned DataFrame and you know 'stops' is ordinal.\n# # You only need to define this one list.\n# ordinal_columns_to_encode = ['stops']\n\n# # Call the function to get a new, fully encoded DataFrame.\n# df_encoded = auto_encode_df(df1, ordinal_columns_to_encode)\n\n# print(\"Original DataFrame columns:\", df1.columns.tolist())\n# print(\"Encoded DataFrame columns:\", df_encoded.columns.tolist())\n# print(\"\\nFirst 5 rows of the encoded DataFrame:\")\n# print(df_encoded.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.450291Z","iopub.execute_input":"2025-08-14T16:39:33.450621Z","iopub.status.idle":"2025-08-14T16:39:33.468984Z","shell.execute_reply.started":"2025-08-14T16:39:33.450600Z","shell.execute_reply":"2025-08-14T16:39:33.467898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import shapiro\nfrom pandas.api.types import is_numeric_dtype\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# This script is designed to work with your DataFrame, df1.\n# Please ensure your df1 DataFrame is loaded and available in your environment\n# before running this code.\n\n# We will create a copy of df1 to store the scaled data\ndf1_scaled = df_encoded.copy()\n\ndef shapiro_test_column(data_series, column_name, alpha=0.05):\n    \"\"\"\n    Performs the Shapiro-Wilk test for normality on a single column.\n    Returns True if the data is likely normal, False otherwise.\n    Includes error handling for data where the test cannot be performed.\n    \"\"\"\n    # Shapiro-Wilk test requires at least 3 samples\n    if len(data_series.dropna()) < 3:\n        print(f\"Skipping '{column_name}': not enough data points for the test.\")\n        return False\n        \n    try:\n        # Perform the Shapiro-Wilk test\n        stat, p_value = shapiro(data_series.dropna())\n        \n        print(f\"\\n--- Results for column: '{column_name}' ---\")\n        print(f\"Shapiro-Wilk Test Statistic: {stat:.4f}\")\n        print(f\"P-value: {p_value:.4f}\")\n        \n        if p_value > alpha:\n            print(\"Conclusion: The data is likely from a normal distribution.\")\n            return True\n        else:\n            print(\"Conclusion: The data is likely NOT from a normal distribution.\")\n            return False\n            \n    except Exception as e:\n        print(f\"An error occurred while testing column '{column_name}': {e}\")\n        return False\n\n# Lists to store the names of columns based on their distribution\nnormal_columns = []\nnon_normal_columns = []\n\nprint(\"Starting normality checks on all columns in your DataFrame (df1)...\\n\")\n\n# Loop through each column in the DataFrame\nfor column in df1.columns:\n    # Check if the column is a numerical type before attempting the test\n    if is_numeric_dtype(df1[column]):\n        is_normal = shapiro_test_column(df1[column], column)\n        \n        if is_normal:\n            normal_columns.append(column)\n        else:\n            non_normal_columns.append(column)\n            \n    else:\n        print(f\"--- Skipping non-numerical column: '{column}' ---\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Normality checks complete.\")\nprint(f\"Columns identified as 'normal' (p-value > 0.05): {normal_columns}\")\nprint(f\"Columns identified as 'not normal' (p-value <= 0.05): {non_normal_columns}\")\nprint(\"=\"*50)\n\n# Apply the appropriate scaling technique to the identified columns\nif normal_columns:\n    print(\"\\nApplying StandardScaler to normal columns...\")\n    scaler_std = StandardScaler()\n    df1_scaled[normal_columns] = scaler_std.fit_transform(df1[normal_columns])\n    print(\"StandardScaler applied successfully.\")\n\nif non_normal_columns:\n    print(\"\\nApplying MinMaxScaler to non-normal columns...\")\n    scaler_minmax = MinMaxScaler()\n    df1_scaled[non_normal_columns] = scaler_minmax.fit_transform(df1[non_normal_columns])\n    print(\"MinMaxScaler applied successfully.\")\n\nprint(\"\\nFirst 5 rows of the scaled DataFrame (df1_scaled):\")\nprint(df1_scaled.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.470037Z","iopub.execute_input":"2025-08-14T16:39:33.470362Z","iopub.status.idle":"2025-08-14T16:39:33.643266Z","shell.execute_reply.started":"2025-08-14T16:39:33.470332Z","shell.execute_reply":"2025-08-14T16:39:33.642307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1_scaled.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.646942Z","iopub.execute_input":"2025-08-14T16:39:33.647237Z","iopub.status.idle":"2025-08-14T16:39:33.664410Z","shell.execute_reply.started":"2025-08-14T16:39:33.647214Z","shell.execute_reply":"2025-08-14T16:39:33.663355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.feature_selection import SelectKBest, f_classif, chi2, f_regression\n# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n# from sklearn.feature_selection import SelectFromModel\n# import numpy as np\n\n# # This script is designed to run on a DataFrame that has already been\n# # preprocessed, including scaling of numerical features and one-hot\n# # encoding of categorical features.\n\n# def perform_automated_feature_selection(df, target_column_name, k_features=10):\n#     \"\"\"\n#     A versatile function to automatically select features for both classification\n#     and regression problems.\n\n#     Args:\n#         df (pd.DataFrame): The input DataFrame.\n#         target_column_name (str): The name of the target variable column.\n#         k_features (int): The number of top features to select using filter methods.\n\n#     Returns:\n#         tuple: A tuple containing the new DataFrame with selected features and\n#                a list of the names of the selected features.\n#     \"\"\"\n#     # --- Step 1: Prepare the Data ---\n#     print(\"Step 1: Preparing data for feature selection...\")\n#     X = df.drop(target_column_name, axis=1)\n#     y = df[target_column_name]\n\n#     # Check if the problem is classification or regression\n#     # Based on the user's input, the target is 'duration', which is numerical.\n#     # Therefore, this is a regression problem.\n#     problem_type = 'regression'\n#     print(\"Detected problem type: Regression (Target variable is numerical)\")\n    \n#     # The target variable 'duration' is already numerical, so no encoding is needed.\n#     y_encoded = y\n    \n#     # Identify numerical and categorical columns from the features\n#     numerical_cols = X.select_dtypes(include=np.number).columns\n#     categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n\n#     # Handle the case where no categorical columns are found\n#     if not categorical_cols.empty:\n#         # One-hot encode categorical features\n#         X_encoded_categorical = pd.get_dummies(X[categorical_cols], drop_first=True)\n#         # Combine numerical and encoded categorical features\n#         X_combined = pd.concat([X[numerical_cols], X_encoded_categorical], axis=1)\n#     else:\n#         print(\"No categorical columns found. Using only numerical features.\")\n#         X_combined = X[numerical_cols]\n\n#     print(f\"Original features: {len(X.columns)}\")\n#     print(f\"Features after one-hot encoding: {len(X_combined.columns)}\")\n#     print(\"-\" * 50)\n\n#     # --- Step 2: Apply Filter Methods ---\n#     print(\"Step 2: Applying filter methods...\")\n    \n#     # Use f_regression for numerical features vs. a numerical target\n#     selector_filter = SelectKBest(score_func=f_regression, k=k_features)\n#     selector_filter.fit(X_combined, y_encoded)\n    \n#     anova_scores = pd.Series(selector_filter.scores_, index=X_combined.columns)\n#     filtered_features = anova_scores.nlargest(k_features).index.tolist()\n\n#     # Note: Chi-Square is not applicable for a regression problem, so we skip it.\n#     print(f\"Top {k_features} features from filter methods (f_regression): {filtered_features}\")\n#     print(\"-\" * 50)\n\n#     # --- Step 3: Apply Embedded Method ---\n#     print(\"Step 3: Applying embedded method...\")\n#     # Use RandomForestRegressor for a regression problem\n#     model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n#     selector_embedded = SelectFromModel(model, threshold='median', prefit=False)\n#     selector_embedded.fit(X_combined, y_encoded)\n#     embedded_features = X_combined.columns[selector_embedded.get_support()].tolist()\n#     print(f\"Features selected by Random Forest: {embedded_features}\")\n#     print(\"-\" * 50)\n\n#     # --- Step 4: Combine Results and Finalize ---\n#     print(\"Step 4: Combining results and finalizing feature list...\")\n#     final_selected_features = list(set(filtered_features + embedded_features))\n#     df_final = X_combined[final_selected_features]\n    \n#     print(f\"Final selected features ({len(final_selected_features)}): {final_selected_features}\")\n#     print(f\"Final DataFrame shape: {df_final.shape}\")\n    \n#     return df_final, final_selected_features\n\n# # --- Execution ---\n# # Call the function to perform feature selection with 'duration' as the target\n# # This assumes 'df1_scaled' is already defined in your environment.\n# df_final_features, selected_features_list = perform_automated_feature_selection(df1_scaled, 'duration', k_features=10)\n\n# # Display the final DataFrame and list of features\n# print(\"\\n--- Final Results ---\")\n# print(\"Selected Features List:\")\n# print(selected_features_list)\n# print(\"\\nFinal DataFrame with selected features (first 5 rows):\")\n# print(df_final_features.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.665349Z","iopub.execute_input":"2025-08-14T16:39:33.665692Z","iopub.status.idle":"2025-08-14T16:39:33.685023Z","shell.execute_reply.started":"2025-08-14T16:39:33.665661Z","shell.execute_reply":"2025-08-14T16:39:33.684103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#magic function to apply feature selction with chi, embedded(reg and classi), filter for reg and classi, \n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2, f_regression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\ndef perform_automated_feature_selection(df, target_column_name, k_features=10):\n    \"\"\"\n    Automatically selects the most important features for a dataset.\n    This function intelligently switches between methods for classification\n    and regression problems.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to analyze. It should already be\n                           preprocessed, with numerical columns scaled and\n                           categorical columns one-hot encoded.\n        target_column_name (str): The name of the column you want to predict.\n        k_features (int): The number of features to select using statistical tests.\n\n    Returns:\n        tuple: A new DataFrame with only the selected features, and a list of\n               the names of those features.\n    \"\"\"\n    # --- Step 1: Prepare the Data (Separate features from target) ---\n    print(\"Step 1: Separating features (X) and target (y)...\")\n    X = df.drop(target_column_name, axis=1)\n    y = df[target_column_name]\n\n    # Automatically decide if it's a classification or regression problem\n    # If the target has few unique values or is an object type, it's classification.\n    if y.dtype == 'object' or y.dtype == 'category' or y.nunique() <= 20:\n        problem_type = 'classification'\n        print(f\"Problem Type: Classification (Target '{target_column_name}' has {y.nunique()} unique values)\")\n        # Encode the target variable for classification models\n        le = LabelEncoder()\n        y_encoded = le.fit_transform(y)\n    else:\n        problem_type = 'regression'\n        print(f\"Problem Type: Regression (Target '{target_column_name}' is numerical)\")\n        y_encoded = y\n    \n    # Identify numerical and categorical columns from the features\n    numerical_cols = X.select_dtypes(include=np.number).columns\n    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n\n    # --- Step 2: Handle Categorical Features and Combine Data ---\n    print(\"Step 2: Handling categorical features and combining data...\")\n    if not categorical_cols.empty:\n        # One-hot encode categorical features if they exist\n        X_encoded_categorical = pd.get_dummies(X[categorical_cols], drop_first=True)\n        X_combined = pd.concat([X[numerical_cols], X_encoded_categorical], axis=1)\n    else:\n        print(\"No categorical columns found. Using only numerical features.\")\n        X_combined = X[numerical_cols]\n\n    print(f\"Features ready for selection. Total columns: {len(X_combined.columns)}\")\n    print(\"-\" * 50)\n    \n    # --- Step 3: Choose Methods Based on Problem Type ---\n    print(\"Step 3: Choosing feature selection methods...\")\n    if problem_type == 'classification':\n        score_func = f_classif\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\n    else: # Regression\n        score_func = f_regression\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n    # --- Step 4: Apply Filter Methods (Statistical Tests) ---\n    print(\"Step 4: Applying statistical filter methods...\")\n    filtered_features = []\n    \n    if problem_type == 'classification' and not categorical_cols.empty:\n        # Use Chi-Square for categorical features in classification\n        selector_chi2 = SelectKBest(score_func=chi2, k='all')\n        selector_chi2.fit(X_combined[X_encoded_categorical.columns], y_encoded)\n        chi2_scores = pd.Series(selector_chi2.scores_, index=X_encoded_categorical.columns)\n        \n        # Use f_classif for numerical features\n        selector_anova = SelectKBest(score_func=score_func, k='all')\n        selector_anova.fit(X_combined[numerical_cols], y_encoded)\n        anova_scores = pd.Series(selector_anova.scores_, index=numerical_cols)\n        \n        # Concatenate scores and get the top K features\n        combined_scores = pd.concat([anova_scores, chi2_scores]).sort_values(ascending=False)\n        filtered_features = combined_scores.head(k_features).index.tolist()\n        print(f\"Top {k_features} features from f_classif and chi-square: {filtered_features}\")\n        \n    else: # This applies to all regression problems and classification without categorical data\n        selector_filter = SelectKBest(score_func=score_func, k=k_features)\n        selector_filter.fit(X_combined, y_encoded)\n        filtered_scores = pd.Series(selector_filter.scores_, index=X_combined.columns)\n        filtered_features = filtered_scores.nlargest(k_features).index.tolist()\n        print(f\"Top {k_features} features from {score_func.__name__}: {filtered_features}\")\n\n    print(\"-\" * 50)\n\n    # --- Step 5: Apply Embedded Method (Random Forest) ---\n    print(\"Step 5: Applying embedded method (Random Forest)...\")\n    selector_embedded = SelectFromModel(model, threshold='median', prefit=False)\n    selector_embedded.fit(X_combined, y_encoded)\n    embedded_features = X_combined.columns[selector_embedded.get_support()].tolist()\n    print(f\"Features selected by Random Forest: {embedded_features}\")\n    print(\"-\" * 50)\n\n    # --- Step 6: Combine Results and Finalize ---\n    print(\"Step 6: Combining results and creating final DataFrame...\")\n    final_selected_features = list(set(filtered_features + embedded_features))\n    df_final = X_combined[final_selected_features]\n    \n    print(f\"Final selected features ({len(final_selected_features)}): {final_selected_features}\")\n    print(f\"Final DataFrame shape: {df_final.shape}\")\n    \n    return df_final, final_selected_features\n\n# --- Execution ---\n# Call the function with your DataFrame and target column name.\n# Replace 'df1_scaled' and 'duration' with your actual variables.\ndf_final_features, selected_features_list = perform_automated_feature_selection(df1_scaled, 'duration', k_features=10)\n\n# Display the final DataFrame and list of features\nprint(\"\\n--- Final Results ---\")\nprint(\"Selected Features List:\")\nprint(selected_features_list)\nprint(\"\\nFinal DataFrame with selected features (first 5 rows):\")\nprint(df_final_features.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:39:33.686050Z","iopub.execute_input":"2025-08-14T16:39:33.686436Z","iopub.status.idle":"2025-08-14T16:42:06.898303Z","shell.execute_reply.started":"2025-08-14T16:39:33.686404Z","shell.execute_reply":"2025-08-14T16:42:06.897424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_final_features.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:42:06.899367Z","iopub.execute_input":"2025-08-14T16:42:06.899631Z","iopub.status.idle":"2025-08-14T16:42:06.908157Z","shell.execute_reply.started":"2025-08-14T16:42:06.899611Z","shell.execute_reply":"2025-08-14T16:42:06.907188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n# from pandas.api.types import is_numeric_dtype\n# from scipy.stats import shapiro\n\n# # --- PART 1: SIMULATE YOUR DATAFRAME AND SCALING ---\n# # This simulates the steps in your workflow that lead to df1_scaled.\n# # The key here is to correctly unpack the result.\n\n# # 1. Create a sample DataFrame to simulate your data.\n# np.random.seed(42)\n# df_original = pd.DataFrame({\n#     'normal_col': np.random.normal(loc=10, scale=2, size=100),\n#     'non_normal_col': np.random.exponential(scale=3, size=100),\n#     'categorical_col': np.random.choice(['A', 'B', 'C'], size=100),\n#     'target_col': np.random.randint(0, 2, size=100)\n# })\n\n# # 2. A function that performs scaling and returns multiple items (a DataFrame and scalers).\n# def apply_scaling_by_normality(df_input, exclude_cols=[]):\n#     \"\"\"\n#     Simulates a scaling function that returns a scaled DataFrame and a scaler model.\n#     \"\"\"\n#     df_output = df_input.copy()\n#     scalers = {}\n\n#     normal_columns = [col for col in df_input.select_dtypes(include=np.number).columns\n#                       if col not in exclude_cols and shapiro(df_input[col].dropna())[1] > 0.05]\n#     non_normal_columns = [col for col in df_input.select_dtypes(include=np.number).columns\n#                           if col not in exclude_cols and shapiro(df_input[col].dropna())[1] <= 0.05]\n\n#     if normal_columns:\n#         scaler_std = StandardScaler()\n#         df_output[normal_columns] = scaler_std.fit_transform(df_input[normal_columns])\n#         scalers['standard'] = scaler_std\n#     else:\n#         # A simple check to ensure a scaler is still available.\n#         # This will only be run if there are no 'normal' columns.\n#         scaler_std = StandardScaler()\n#         scalers['standard'] = scaler_std\n    \n#     if non_normal_columns:\n#         scaler_minmax = MinMaxScaler()\n#         df_output[non_normal_columns] = scaler_minmax.fit_transform(df_input[non_normal_columns])\n#         scalers['minmax'] = scaler_minmax\n#     else:\n#         # A simple check to ensure a scaler is still available.\n#         scaler_minmax = MinMaxScaler()\n#         scalers['minmax'] = scaler_minmax\n\n\n#     # The return value is a tuple of two items.\n#     return df_output, scalers\n\n# # 3. The CORRECT way to call the scaling function.\n# # We use two variables to \"unpack\" the two items returned by the function.\n# # This ensures that df1_scaled is a DataFrame, not a tuple.\n# df1_scaled, scaler_models = apply_scaling_by_normality(df_original, exclude_cols=['target_col'])\n# print(\"Successfully unpacked scaling function output.\")\n# print(f\"Type of df1_scaled: {type(df1_scaled)}\")\n# print(\"=\"*50)\n\n\n# # --- PART 2: THE PCA FUNCTION WITH AN EXPLICIT ERROR CHECK ---\n\n# def apply_pca(df, n_components=0.95):\n#     \"\"\"\n#     Apply PCA retaining n_components variance (float) or number of components (int).\n#     \"\"\"\n#     # --- CRITICAL FIX: Explicitly check if the input is a DataFrame ---\n#     if not isinstance(df, pd.DataFrame):\n#         print(f\"ERROR: Expected a pandas DataFrame, but received a '{type(df).__name__}' object.\")\n#         print(\"This means a previous function call was not unpacked correctly.\")\n#         print(\"Please ensure the scaling function output is assigned to two variables, e.g., 'df_scaled, scaler_model = scale_function(...)'.\")\n#         # Returning here to prevent the code from crashing.\n#         return None, None\n    \n#     # This line will now work because 'df' is a pandas DataFrame.\n#     df_numerical = df.select_dtypes(include=np.number)\n    \n#     # We'll drop any categorical or non-numerical columns here to be safe\n#     df_numerical = df.select_dtypes(include=np.number).drop(columns=['target_col'], errors='ignore')\n    \n#     if df_numerical.shape[1] < 2:\n#         print(\"Not enough numerical columns for PCA.\")\n#         return df_numerical, None\n\n#     pca = PCA(n_components=n_components)\n#     components = pca.fit_transform(df_numerical)\n    \n#     col_names = [f'PC{i+1}' for i in range(components.shape[1])]\n#     df_pca = pd.DataFrame(components, columns=col_names, index=df_numerical.index)\n    \n#     print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n#     print(f\"Number of components selected: {pca.n_components_}\")\n#     print(f\"Original shape: {df_numerical.shape}\")\n#     print(f\"PCA-transformed shape: {df_pca.shape}\")\n    \n#     return df_pca, pca\n\n# # --- CORRECTED CALL: This line will now execute without the TypeError ---\n# # We are passing the correctly unpacked DataFrame to the function.\n# df1_dimensionality, pca_model = apply_pca(df1_scaled, n_components=0.95)\n\n# if df1_dimensionality is not None:\n#     print(\"\\nPCA applied successfully. The new DataFrame is ready.\")\n#     print(df1_dimensionality.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:42:06.909053Z","iopub.execute_input":"2025-08-14T16:42:06.909265Z","iopub.status.idle":"2025-08-14T16:42:06.928063Z","shell.execute_reply.started":"2025-08-14T16:42:06.909249Z","shell.execute_reply":"2025-08-14T16:42:06.926892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_final_features.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:42:06.929024Z","iopub.execute_input":"2025-08-14T16:42:06.929606Z","iopub.status.idle":"2025-08-14T16:42:06.956331Z","shell.execute_reply.started":"2025-08-14T16:42:06.929582Z","shell.execute_reply":"2025-08-14T16:42:06.955486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_final_features.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:42:06.957217Z","iopub.execute_input":"2025-08-14T16:42:06.957464Z","iopub.status.idle":"2025-08-14T16:42:06.977188Z","shell.execute_reply.started":"2025-08-14T16:42:06.957445Z","shell.execute_reply":"2025-08-14T16:42:06.976437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'df1_scaled' is your full, preprocessed DataFrame with the 'duration' column.\n# The `perform_automated_feature_selection` function returns a DataFrame with only the selected features.\ndf_final_features, selected_features_list = perform_automated_feature_selection(df1_scaled, 'duration', k_features=10)\n\n# --- Define X and y for the train-test split ---\n# X will be the DataFrame of selected features returned from your function.\nX = df_final_features\n\n# y will be the original target column from your starting DataFrame.\ny = df1_scaled['duration']\n\n# --- Perform the train-test split ---\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=42\n)\n\nprint(f\"Training features (X_train) shape: {X_train.shape}\")\nprint(f\"Testing features (X_test) shape: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:42:06.978075Z","iopub.execute_input":"2025-08-14T16:42:06.978318Z","iopub.status.idle":"2025-08-14T16:44:37.595841Z","shell.execute_reply.started":"2025-08-14T16:42:06.978297Z","shell.execute_reply":"2025-08-14T16:44:37.594698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.metrics import mean_absolute_error, r2_score\n\n# # Regression Models\n# from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n# from sklearn.svm import SVR\n# from sklearn.neighbors import KNeighborsRegressor\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.pipeline import Pipeline\n# from sklearn.neural_network import MLPRegressor\n# from xgboost import XGBRegressor\n# from lightgbm import LGBMRegressor\n# from catboost import CatBoostRegressor\n\n# # ===============================================================================\n# # This script assumes you have already defined and split your data into\n# # X_train, X_test, y_train, and y_test.\n# # You can now proceed directly with model training and evaluation.\n# # ===============================================================================\n\n# # Define a dictionary of all the regression models to test\n# models = {\n#     \"Linear Regression\": LinearRegression(),\n#     \"Ridge Regression\": RidgeCV(),\n#     \"Lasso Regression\": LassoCV(),\n#     \"ElasticNet Regression\": ElasticNetCV(),\n#     \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n#     \"Random Forest\": RandomForestRegressor(random_state=42),\n#     \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n#     \"AdaBoost\": AdaBoostRegressor(random_state=42),\n#     \"XGBoost\": XGBRegressor(random_state=42),\n#     \"LightGBM\": LGBMRegressor(random_state=42),\n#     \"CatBoost\": CatBoostRegressor(random_state=42, verbose=0),\n#     \"SVR\": SVR(),\n#     \"K-Nearest Neighbors\": KNeighborsRegressor(),\n#     \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=500),\n#     # Using PolynomialFeatures requires a pipeline\n#     \"Polynomial Regression\": Pipeline([\n#         ('poly', PolynomialFeatures(degree=2)),\n#         ('linreg', LinearRegression())\n#     ])\n# }\n\n# # Loop through each model, train it, and evaluate its performance\n# print(\"\\n--- Starting Model Training and Evaluation ---\")\n# for name, model in models.items():\n#     print(f\"\\nTraining and evaluating: {name}...\")\n#     try:\n#         # Train the model\n#         model.fit(X_train, y_train)\n\n#         # Make predictions on the test set\n#         y_pred = model.predict(X_test)\n\n#         # Calculate and print performance metrics\n#         mae = mean_absolute_error(y_test, y_pred)\n#         r2 = r2_score(y_test, y_pred)\n\n#         print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n#         print(f\"  R-squared (R): {r2:.4f}\")\n\n#     except Exception as e:\n#         print(f\"  Error with {name}: {e}\")\n#     finally:\n#         print(\"-\" * 50)\n\n# print(\"All models have been evaluated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:44:37.597094Z","iopub.execute_input":"2025-08-14T16:44:37.597467Z","iopub.status.idle":"2025-08-14T16:44:37.603924Z","shell.execute_reply.started":"2025-08-14T16:44:37.597436Z","shell.execute_reply":"2025-08-14T16:44:37.602921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# Import necessary models for the first set\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n\n# ===============================================================================\n# Run this code in a separate cell.\n# It assumes X_train, X_test, y_train, and y_test are already defined.\n# ===============================================================================\n\n# Define a dictionary of the first set of regression models to test\nmodels_set1 = {\n    \"Linear Regression\": LinearRegression(),\n    \"Ridge Regression\": RidgeCV(),\n    \"Lasso Regression\": LassoCV(),\n    \"ElasticNet Regression\": ElasticNetCV(),\n    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n    \"Random Forest\": RandomForestRegressor(random_state=42),\n    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n    \"AdaBoost\": AdaBoostRegressor(random_state=42),\n}\n\n# Loop through each model, train it, and evaluate its performance\nprint(\"\\n--- Starting Model Training and Evaluation (Set 1) ---\")\nfor name, model in models_set1.items():\n    print(f\"\\nTraining and evaluating: {name}...\")\n    try:\n        # Train the model\n        model.fit(X_train, y_train)\n\n        # Make predictions on the test set\n        y_pred = model.predict(X_test)\n\n        # Calculate and print performance metrics\n        mae = mean_absolute_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n\n        mape = np.mean(np.abs((y_test - y_pred) / y_test.replace(0, 1e-10))) * 100\n\n        print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n        print(f\"  R-squared (R): {r2:.4f}\")\n        print(f\"  Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n\n    \n\n    except Exception as e:\n        print(f\"  Error with {name}: {e}\")\n    finally:\n        print(\"-\" * 50)\n\nprint(\"Set 1 models have been evaluated.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:48:41.059256Z","iopub.execute_input":"2025-08-14T16:48:41.060053Z","iopub.status.idle":"2025-08-14T16:51:56.877106Z","shell.execute_reply.started":"2025-08-14T16:48:41.060021Z","shell.execute_reply":"2025-08-14T16:51:56.875957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# Import necessary models for the second set\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# ===============================================================================\n# Run this code in a separate cell.\n# It assumes X_train, X_test, y_train, and y_test are already defined.\n# ===============================================================================\n\n# Define a dictionary of the second set of regression models to test\nmodels_set2 = {\n    \"XGBoost\": XGBRegressor(random_state=42),\n    \"LightGBM\": LGBMRegressor(random_state=42),\n    \"CatBoost\": CatBoostRegressor(random_state=42, verbose=0),\n    # \"SVR\": SVR(),\n    \"K-Nearest Neighbors\": KNeighborsRegressor(),\n    \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=500),\n    # Using PolynomialFeatures requires a pipeline\n    \"Polynomial Regression\": Pipeline([\n        ('poly', PolynomialFeatures(degree=2)),\n        ('linreg', LinearRegression())\n    ])\n}\n\n# Loop through each model, train it, and evaluate its performance\nprint(\"\\n--- Starting Model Training and Evaluation (Set 2) ---\")\nfor name, model in models_set2.items():\n    print(f\"\\nTraining and evaluating: {name}...\")\n    try:\n        # Train the model\n        model.fit(X_train, y_train)\n\n        # Make predictions on the test set\n        y_pred = model.predict(X_test)\n\n        # Calculate and print performance metrics\n        mae = mean_absolute_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n\n        mape = np.mean(np.abs((y_test - y_pred) / y_test.replace(0, 1e-10))) * 100\n\n        print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n        print(f\"  R-squared (R): {r2:.4f}\")\n        print(f\"  Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n\n    except Exception as e:\n        print(f\"  Error with {name}: {e}\")\n    finally:\n        print(\"-\" * 50)\n\nprint(\"Set 2 models have been evaluated.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:52:53.034093Z","iopub.execute_input":"2025-08-14T16:52:53.034405Z","iopub.status.idle":"2025-08-14T16:54:29.590424Z","shell.execute_reply.started":"2025-08-14T16:52:53.034367Z","shell.execute_reply":"2025-08-14T16:54:29.589452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import all necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_validate, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Assume df_final_features and df1_scaled['duration'] are already defined\n# X = df_final_features\n# y = df1_scaled['duration']\n\n# --- Step 1: Split data into training and testing sets ---\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# --- Step 2: Create a smaller training sample for SVR from the existing training data ---\n# SVR can be slow on large datasets. This creates a 10% subsample of the\n# training data specifically for the SVR model.\nX_train_svr, _, y_train_svr, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\nprint(f\"Created a smaller training sample for SVR with {len(X_train_svr)} observations.\")\n\n# --- Step 3: Define Preprocessing ---\n# Identify numeric and categorical features\nnumeric_features = X.select_dtypes(include=np.number).columns\ncategorical_features = X.select_dtypes(include='object').columns\n\n# Create preprocessing steps for numeric and categorical data\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Use ColumnTransformer to apply the transformations to the correct columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n\n# --- Step 4: Define custom evaluation functions ---\ndef mape(y_true, y_pred):\n    \"\"\"\n    Calculates the Mean Absolute Percentage Error (MAPE).\n    \"\"\"\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-10\n    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\ndef rmsle(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Squared Logarithmic Error (RMSLE).\n    \"\"\"\n    # Make sure predictions are non-negative and add 1 for log\n    y_pred[y_pred < 0] = 0\n    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n\ndef adj_r2(y_true, y_pred, n_features):\n    \"\"\"\n    Calculates the Adjusted R-squared.\n    n is the number of observations, p is the number of predictors.\n    \"\"\"\n    n = y_true.shape[0]\n    p = n_features\n    r2 = r2_score(y_true, y_pred)\n    if p >= n - 1: # Avoid division by zero if p is too large\n        return np.nan\n    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n\n\n# --- Step 5: Hyperparameter Tuning ---\nprint(\"\\n--- Performing Hyperparameter Tuning for selected models ---\")\n\n# --- a) Decision Tree Regressor (Grid Search) ---\nprint(\"\\n  > Starting Grid Search for Decision Tree...\")\ndt_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))])\nparam_grid_dt = {\n    'regressor__max_depth': [5, 10, 15],\n    'regressor__min_samples_leaf': [1, 5, 10],\n    'regressor__criterion': ['squared_error', 'absolute_error']\n}\ngrid_search_dt = GridSearchCV(dt_pipeline, param_grid_dt, cv=5, scoring='r2', n_jobs=-1, verbose=1)\ngrid_search_dt.fit(X_train, y_train)\nbest_dt_model = grid_search_dt.best_estimator_\nprint(f\"  > Best Decision Tree parameters: {grid_search_dt.best_params_}\")\n\n\n# --- b) SVR (Grid Search) ---\nprint(\"\\n  > Starting Grid Search for SVR...\")\nsvr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR())])\nparam_grid_svr = {\n    'regressor__kernel': ['linear', 'rbf'],\n    'regressor__C': [0.1, 1, 10],\n    'regressor__epsilon': [0.1, 0.2]\n}\ngrid_search_svr = GridSearchCV(svr_pipeline, param_grid_svr, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n# Use the smaller subsample for SVR tuning\ngrid_search_svr.fit(X_train_svr, y_train_svr)\nbest_svr_model = grid_search_svr.best_estimator_\nprint(f\"  > Best SVR parameters: {grid_search_svr.best_params_}\")\n\n\n# --- c) Random Forest Regressor (Randomized Search) ---\nprint(\"\\n  > Starting Randomized Search for Random Forest...\")\nrf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(random_state=42))])\nparam_distributions_rf = {\n    'regressor__n_estimators': [100, 200, 300],\n    'regressor__max_depth': [10, 20, None],\n    'regressor__min_samples_leaf': [1, 2, 4],\n    'regressor__min_samples_split': [2, 5, 10]\n}\nrandom_search_rf = RandomizedSearchCV(rf_pipeline, param_distributions_rf, n_iter=10, cv=3, scoring='r2', n_jobs=-1, verbose=1, random_state=42)\nrandom_search_rf.fit(X_train, y_train)\nbest_rf_model = random_search_rf.best_estimator_\nprint(f\"  > Best Random Forest parameters: {random_search_rf.best_params_}\")\n\n\n# --- d) Gradient Boosting Regressor (Randomized Search) ---\nprint(\"\\n  > Starting Randomized Search for Gradient Boosting...\")\ngb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))])\nparam_distributions_gb = {\n    'regressor__n_estimators': [100, 200, 300],\n    'regressor__learning_rate': [0.01, 0.1, 0.2],\n    'regressor__max_depth': [3, 5, 8],\n    'regressor__subsample': [0.8, 0.9, 1.0]\n}\nrandom_search_gb = RandomizedSearchCV(gb_pipeline, param_distributions_gb, n_iter=10, cv=3, scoring='r2', n_jobs=-1, verbose=1, random_state=42)\nrandom_search_gb.fit(X_train, y_train)\nbest_gb_model = random_search_gb.best_estimator_\nprint(f\"  > Best Gradient Boosting parameters: {random_search_gb.best_params_}\")\n\n\n# --- e) XGBoost Regressor (Randomized Search) ---\nprint(\"\\n  > Starting Randomized Search for XGBoost...\")\nxgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(random_state=42, objective='reg:squarederror'))])\nparam_distributions_xgb = {\n    'regressor__n_estimators': [100, 200, 300],\n    'regressor__learning_rate': [0.01, 0.1, 0.2],\n    'regressor__max_depth': [3, 5, 8],\n    'regressor__subsample': [0.8, 0.9, 1.0]\n}\nrandom_search_xgb = RandomizedSearchCV(xgb_pipeline, param_distributions_xgb, n_iter=10, cv=3, scoring='r2', n_jobs=-1, verbose=1, random_state=42)\nrandom_search_xgb.fit(X_train, y_train)\nbest_xgb_model = random_search_xgb.best_estimator_\nprint(f\"  > Best XGBoost parameters: {random_search_xgb.best_params_}\")\n\n\n# --- Step 6: Create a dictionary of all regression models (tuned and untuned) ---\n# This dictionary now includes all our best-performing, tuned models for a\n# comprehensive comparison against the baseline models.\nmodels = {\n    'Linear Regression': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())]),\n    'RidgeCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RidgeCV(alphas=np.logspace(-6, 6, 13)))]),\n    'LassoCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LassoCV(alphas=np.logspace(-6, 6, 13), cv=5))]),\n    'ElasticNetCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', ElasticNetCV(alphas=np.logspace(-6, 6, 13), cv=5))]),\n    'Decision Tree Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))]),\n    'Tuned Decision Tree Regressor': best_dt_model,\n    'Random Forest Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(random_state=42))]),\n    'Tuned Random Forest Regressor': best_rf_model,\n    'Gradient Boosting Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))]),\n    'Tuned Gradient Boosting Regressor': best_gb_model,\n    'AdaBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', AdaBoostRegressor(random_state=42))]),\n    'SVR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR())]),\n    'Tuned SVR': best_svr_model,\n    'KNeighbors Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_neighbors=5))]),\n    'MLP Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42))]),\n    'XGB Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))]),\n    'Tuned XGB Regressor': best_xgb_model,\n    'LGBM Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LGBMRegressor(random_state=42))]),\n    'CatBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', CatBoostRegressor(verbose=0, random_state=42))])\n}\n\n# --- Step 7: Train and Evaluate all models in a loop ---\nprint(\"\\n--- Training and Evaluating all Regression Models ---\")\n\n# First, get the number of features after preprocessing to calculate Adjusted R2\nX_train_transformed = preprocessor.fit_transform(X_train)\nn_features = X_train_transformed.shape[1]\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        if 'SVR' in name:\n            # Use the subsampled data for SVR\n            model.fit(X_train_svr, y_train_svr)\n            predictions = model.predict(X_test)\n        else:\n            # Use the full training data for all other models\n            model.fit(X_train, y_train)\n            predictions = model.predict(X_test)\n        \n        # Calculate all requested metrics\n        mse = mean_squared_error(y_test, predictions)\n        mae = mean_absolute_error(y_test, predictions)\n        rmse = np.sqrt(mse)\n        rmsle_score = rmsle(y_test, predictions)\n        r2 = r2_score(y_test, predictions)\n        adj_r2_score = adj_r2(y_test, predictions, n_features)\n        mape_score = mape(y_test, predictions)\n        \n        print(f\"--- {name} Results ---\")\n        print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n        print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n        print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n        print(f\"Root Mean Squared Logarithmic Error (RMSLE): {rmsle_score:.4f}\")\n        print(f\"R-squared (R): {r2:.4f}\")\n        print(f\"Adjusted R-squared (Adj_R): {adj_r2_score:.4f}\")\n        print(f\"Mean Absolute Percentage Error (MAPE): {mape_score:.2f}%\")\n        \n    except Exception as e:\n        print(f\"An error occurred while training {name}: {e}\")\n\nprint(\"\\n--- All models evaluated. ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T18:15:11.466798Z","iopub.execute_input":"2025-08-14T18:15:11.467991Z","iopub.status.idle":"2025-08-14T18:27:10.816173Z","shell.execute_reply.started":"2025-08-14T18:15:11.467956Z","shell.execute_reply":"2025-08-14T18:27:10.815010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Import all necessary libraries\n# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split, GridSearchCV\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n# from sklearn.svm import SVR\n# from sklearn.neighbors import KNeighborsRegressor\n# from sklearn.neural_network import MLPRegressor\n# from xgboost import XGBRegressor\n# from lightgbm import LGBMRegressor\n# from catboost import CatBoostRegressor\n# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# # Assume df_final_features and df1_scaled['duration'] are already defined\n# # X = df_final_features\n# # y = df1_scaled['duration']\n\n# # --- Step 1: Split data into training and testing sets ---\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # --- Step 2: Create a smaller training sample for SVR ---\n# X_train_svr, _, y_train_svr, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n# print(f\"Created a smaller training sample for SVR with {len(X_train_svr)} observations.\")\n\n# # --- Step 3: Define Preprocessing ---\n# numeric_features = X.select_dtypes(include=np.number).columns\n# categorical_features = X.select_dtypes(include='object').columns\n\n# numeric_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numeric_transformer, numeric_features),\n#         ('cat', categorical_transformer, categorical_features)\n#     ])\n\n\n# # --- Step 4: Define custom evaluation functions ---\n# def mape(y_true, y_pred):\n#     \"\"\"Calculates the Mean Absolute Percentage Error (MAPE).\"\"\"\n#     epsilon = 1e-10\n#     return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\n# def rmsle(y_true, y_pred):\n#     \"\"\"Calculates the Root Mean Squared Logarithmic Error (RMSLE).\"\"\"\n#     y_pred[y_pred < 0] = 0\n#     return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n\n# def adj_r2(y_true, y_pred, n_features):\n#     \"\"\"Calculates the Adjusted R-squared.\"\"\"\n#     n = y_true.shape[0]\n#     p = n_features\n#     r2 = r2_score(y_true, y_pred)\n#     if p >= n - 1:\n#         return np.nan\n#     return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n\n\n# # --- Step 5: Hyperparameter Tuning (Deep Dive) ---\n# print(\"\\n--- Performing Deep Hyperparameter Tuning for top models ---\")\n\n# # --- a) Random Forest Regressor (Grid Search) ---\n# print(\"\\n  > Starting Grid Search for Random Forest...\")\n# rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(random_state=42))])\n# # Focused and narrowed parameter grid for a more exhaustive search\n# param_grid_rf = {\n#     'regressor__n_estimators': [200, 300],\n#     'regressor__max_depth': [20, 25],\n#     'regressor__min_samples_leaf': [2, 3]\n# }\n# grid_search_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n# grid_search_rf.fit(X_train, y_train)\n# best_rf_model = grid_search_rf.best_estimator_\n# print(f\"  > Best Random Forest parameters: {grid_search_rf.best_params_}\")\n\n\n# # --- b) Gradient Boosting Regressor (Grid Search) ---\n# print(\"\\n  > Starting Grid Search for Gradient Boosting...\")\n# gb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))])\n# # Focused and narrowed parameter grid for a more exhaustive search\n# param_grid_gb = {\n#     'regressor__n_estimators': [200, 300],\n#     'regressor__learning_rate': [0.05, 0.1],\n#     'regressor__max_depth': [5, 7],\n# }\n# grid_search_gb = GridSearchCV(gb_pipeline, param_grid_gb, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n# grid_search_gb.fit(X_train, y_train)\n# best_gb_model = grid_search_gb.best_estimator_\n# print(f\"  > Best Gradient Boosting parameters: {grid_search_gb.best_params_}\")\n\n\n# # --- c) XGBoost Regressor (Grid Search) ---\n# print(\"\\n  > Starting Grid Search for XGBoost...\")\n# xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(random_state=42, objective='reg:squarederror'))])\n# # Focused and narrowed parameter grid for a more exhaustive search\n# param_grid_xgb = {\n#     'regressor__n_estimators': [200, 300],\n#     'regressor__learning_rate': [0.05, 0.1],\n#     'regressor__max_depth': [5, 7],\n# }\n# grid_search_xgb = GridSearchCV(xgb_pipeline, param_grid_xgb, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n# grid_search_xgb.fit(X_train, y_train)\n# best_xgb_model = grid_search_xgb.best_estimator_\n# print(f\"  > Best XGBoost parameters: {grid_search_xgb.best_params_}\")\n\n\n# # --- Step 6: Create a dictionary of all regression models (tuned and untuned) ---\n# # Now we compare the deeply tuned models against the baseline models.\n# models = {\n#     'Linear Regression': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())]),\n#     'RidgeCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RidgeCV(alphas=np.logspace(-6, 6, 13)))]),\n#     'LassoCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LassoCV(alphas=np.logspace(-6, 6, 13), cv=5))]),\n#     'ElasticNetCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', ElasticNetCV(alphas=np.logspace(-6, 6, 13), cv=5))]),\n#     'Random Forest Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(random_state=42))]),\n#     'Tuned Random Forest Regressor': best_rf_model,\n#     'Gradient Boosting Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))]),\n#     'Tuned Gradient Boosting Regressor': best_gb_model,\n#     'XGB Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))]),\n#     'Tuned XGB Regressor': best_xgb_model,\n#     'SVR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR())]),\n#     'KNeighbors Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_neighbors=5))]),\n#     'AdaBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', AdaBoostRegressor(random_state=42))]),\n#     'Decision Tree Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))]),\n#     'LGBM Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LGBMRegressor(random_state=42))]),\n#     'CatBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', CatBoostRegressor(verbose=0, random_state=42))])\n# }\n\n\n# # --- Step 7: Train and Evaluate all models in a loop ---\n# print(\"\\n--- Training and Evaluating all Regression Models ---\")\n\n# # First, get the number of features after preprocessing to calculate Adjusted R2\n# X_train_transformed = preprocessor.fit_transform(X_train)\n# n_features = X_train_transformed.shape[1]\n\n# for name, model in models.items():\n#     print(f\"\\nTraining {name}...\")\n#     try:\n#         if 'SVR' in name:\n#             # Use the subsampled data for SVR\n#             model.fit(X_train_svr, y_train_svr)\n#             predictions = model.predict(X_test)\n#         else:\n#             # Use the full training data for all other models\n#             model.fit(X_train, y_train)\n#             predictions = model.predict(X_test)\n        \n#         # Calculate all requested metrics\n#         mse = mean_squared_error(y_test, predictions)\n#         mae = mean_absolute_error(y_test, predictions)\n#         rmse = np.sqrt(mse)\n#         rmsle_score = rmsle(y_test, predictions)\n#         r2 = r2_score(y_test, predictions)\n#         adj_r2_score = adj_r2(y_test, predictions, n_features)\n#         mape_score = mape(y_test, predictions)\n        \n#         print(f\"--- {name} Results ---\")\n#         print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n#         print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n#         print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n#         print(f\"Root Mean Squared Logarithmic Error (RMSLE): {rmsle_score:.4f}\")\n#         print(f\"R-squared (R): {r2:.4f}\")\n#         print(f\"Adjusted R-squared (Adj_R): {adj_r2_score:.4f}\")\n#         print(f\"Mean Absolute Percentage Error (MAPE): {mape_score:.2f}%\")\n        \n#     except Exception as e:\n#         print(f\"An error occurred while training {name}: {e}\")\n\n# print(\"\\n--- All models evaluated. ---\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import all necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom scipy.stats import randint, uniform\n\n# Assume df_final_features and df1_scaled['duration'] are already defined\n# X = df_final_features\n# y = df1_scaled['duration']\n\n# --- Step 1: Split data into training and testing sets ---\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# --- Step 2: Create a smaller training sample for SVR ---\nX_train_svr, _, y_train_svr, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\nprint(f\"Created a smaller training sample for SVR with {len(X_train_svr)} observations.\")\n\n# --- Step 3: Define Preprocessing ---\nnumeric_features = X.select_dtypes(include=np.number).columns\ncategorical_features = X.select_dtypes(include='object').columns\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n\n# --- Step 4: Define custom evaluation functions ---\ndef mape(y_true, y_pred):\n    \"\"\"Calculates the Mean Absolute Percentage Error (MAPE).\"\"\"\n    epsilon = 1e-10\n    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\ndef rmsle(y_true, y_pred):\n    \"\"\"Calculates the Root Mean Squared Logarithmic Error (RMSLE).\"\"\"\n    y_pred[y_pred < 0] = 0\n    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n\ndef adj_r2(y_true, y_pred, n_features):\n    \"\"\"Calculates the Adjusted R-squared.\"\"\"\n    n = y_true.shape[0]\n    p = n_features\n    r2 = r2_score(y_true, y_pred)\n    if p >= n - 1:\n        return np.nan\n    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n\n\n# --- Step 5: Hyperparameter Tuning (Faster Randomized Search) ---\nprint(\"\\n--- Performing Faster Randomized Hyperparameter Tuning for top models ---\")\n\n# Define parameter distributions for RandomizedSearchCV\nparam_dist_rf = {\n    'regressor__n_estimators': randint(100, 500),\n    'regressor__max_depth': randint(10, 30),\n    'regressor__min_samples_leaf': randint(1, 5)\n}\n\nparam_dist_gb = {\n    'regressor__n_estimators': randint(100, 500),\n    'regressor__learning_rate': uniform(0.01, 0.2),\n    'regressor__max_depth': randint(3, 10),\n}\n\nparam_dist_xgb = {\n    'regressor__n_estimators': randint(100, 500),\n    'regressor__learning_rate': uniform(0.01, 0.2),\n    'regressor__max_depth': randint(3, 10),\n}\n\n# --- a) Random Forest Regressor (Randomized Search) ---\nprint(\"\\n  > Starting Randomized Search for Random Forest...\")\nrf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(random_state=42))])\nrandom_search_rf = RandomizedSearchCV(rf_pipeline, param_dist_rf, n_iter=50, cv=3, scoring='r2', n_jobs=-1, verbose=1, random_state=42)\nrandom_search_rf.fit(X_train, y_train)\nbest_rf_model = random_search_rf.best_estimator_\nprint(f\"  > Best Random Forest parameters: {random_search_rf.best_params_}\")\n\n\n# --- b) Gradient Boosting Regressor (Randomized Search) ---\nprint(\"\\n  > Starting Randomized Search for Gradient Boosting...\")\ngb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))])\nrandom_search_gb = RandomizedSearchCV(gb_pipeline, param_dist_gb, n_iter=50, cv=3, scoring='r2', n_jobs=-1, verbose=1, random_state=42)\nrandom_search_gb.fit(X_train, y_train)\nbest_gb_model = random_search_gb.best_estimator_\nprint(f\"  > Best Gradient Boosting parameters: {random_search_gb.best_params_}\")\n\n\n# --- c) XGBoost Regressor (Randomized Search) ---\nprint(\"\\n  > Starting Randomized Search for XGBoost...\")\nxgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(random_state=42, objective='reg:squarederror'))])\nrandom_search_xgb = RandomizedSearchCV(xgb_pipeline, param_dist_xgb, n_iter=50, cv=3, scoring='r2', n_jobs=-1, verbose=1, random_state=42)\nrandom_search_xgb.fit(X_train, y_train)\nbest_xgb_model = random_search_xgb.best_estimator_\nprint(f\"  > Best XGBoost parameters: {random_search_xgb.best_params_}\")\n\n\n# --- Step 6: Create a dictionary of all regression models (tuned and untuned) ---\nmodels = {\n    'Linear Regression': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())]),\n    'RidgeCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RidgeCV(alphas=np.logspace(-6, 6, 13)))]),\n    'LassoCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LassoCV(alphas=np.logspace(-6, 6, 13), cv=5))]),\n    'ElasticNetCV': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', ElasticNetCV(alphas=np.logspace(-6, 6, 13), cv=5))]),\n    'Random Forest Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(random_state=42))]),\n    'Tuned Random Forest Regressor': best_rf_model,\n    'Gradient Boosting Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))]),\n    'Tuned Gradient Boosting Regressor': best_gb_model,\n    'XGB Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))]),\n    'Tuned XGB Regressor': best_xgb_model,\n    'SVR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR())]),\n    'KNeighbors Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_neighbors=5))]),\n    'AdaBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', AdaBoostRegressor(random_state=42))]),\n    'Decision Tree Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))]),\n    'LGBM Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LGBMRegressor(random_state=42))]),\n    'CatBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', CatBoostRegressor(verbose=0, random_state=42))])\n}\n\n\n# --- Step 7: Train and Evaluate all models in a loop ---\nprint(\"\\n--- Training and Evaluating all Regression Models ---\")\n\n# First, get the number of features after preprocessing to calculate Adjusted R2\nX_train_transformed = preprocessor.fit_transform(X_train)\nn_features = X_train_transformed.shape[1]\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        if 'SVR' in name:\n            # Use the subsampled data for SVR\n            model.fit(X_train_svr, y_train_svr)\n            predictions = model.predict(X_test)\n        else:\n            # Use the full training data for all other models\n            model.fit(X_train, y_train)\n            predictions = model.predict(X_test)\n        \n        # Calculate all requested metrics\n        mse = mean_squared_error(y_test, predictions)\n        mae = mean_absolute_error(y_test, predictions)\n        rmse = np.sqrt(mse)\n        rmsle_score = rmsle(y_test, predictions)\n        r2 = r2_score(y_test, predictions)\n        adj_r2_score = adj_r2(y_test, predictions, n_features)\n        mape_score = mape(y_test, predictions)\n        \n        print(f\"--- {name} Results ---\")\n        print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n        print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n        print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n        print(f\"Root Mean Squared Logarithmic Error (RMSLE): {rmsle_score:.4f}\")\n        print(f\"R-squared (R): {r2:.4f}\")\n        print(f\"Adjusted R-squared (Adj_R): {adj_r2_score:.4f}\")\n        print(f\"Mean Absolute Percentage Error (MAPE): {mape_score:.2f}%\")\n        \n    except Exception as e:\n        print(f\"An error occurred while training {name}: {e}\")\n\nprint(\"\\n--- All models evaluated. ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T18:27:27.663120Z","iopub.execute_input":"2025-08-14T18:27:27.663490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# --- 1. Create a sample dataset for demonstration ---\n# In your actual project, you would load your cleaned and processed X and y data here.\n# X_processed should be your feature-selected, scaled, and encoded training data.\n# y should be your target variable.\nnp.random.seed(42)\nX = np.random.rand(100, 10) * 100\ny = (X[:, 0] > 50).astype(int)  # A simple binary target for demonstration purposes\n\n# --- 2. Split the data into training and testing sets ---\n# It is crucial to split the data *before* tuning to prevent data leakage.\n# Grid search will be performed only on the training set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# --- 3. Define the model you want to tune ---\n# This is where you would place your individual model instance.\n# For this example, we use a Support Vector Classifier (SVC).\nmodel = SVC()\n\n# --- 4. Define the hyperparameter grid ---\n# This dictionary specifies the parameters to tune and the values to test for each.\n# For SVC, we will tune the 'C' and 'kernel' parameters.\n# 'C' is a regularization parameter.\n# 'kernel' is the function used to map data into a higher-dimensional space.\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['linear', 'rbf']\n}\n\n# --- 5. Set up and run the Grid Search ---\n# GridSearchCV will train a model for every combination of parameters in the grid.\n# cv=5 means it will use 5-fold cross-validation on the training data.\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n\n# Fit the grid search to the training data.\n# This will perform the exhaustive search for the best parameters.\nprint(\"Starting Grid Search...\")\ngrid_search.fit(X_train, y_train)\n\n# --- 6. Review the results ---\nprint(\"\\n--- Tuning Results ---\")\nprint(f\"Best parameters found: {grid_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n\n# --- 7. Evaluate the best model on the test data ---\n# We use the best model found by the grid search and apply it to the unseen test data.\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Accuracy of the best model on the test set: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}