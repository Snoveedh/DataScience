{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12567314,"sourceType":"datasetVersion","datasetId":7936316}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:04.104595Z","iopub.execute_input":"2025-08-18T10:50:04.104975Z","iopub.status.idle":"2025-08-18T10:50:04.463754Z","shell.execute_reply.started":"2025-08-18T10:50:04.104946Z","shell.execute_reply":"2025-08-18T10:50:04.462839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:04.465506Z","iopub.execute_input":"2025-08-18T10:50:04.466017Z","iopub.status.idle":"2025-08-18T10:50:05.834033Z","shell.execute_reply.started":"2025-08-18T10:50:04.465993Z","shell.execute_reply":"2025-08-18T10:50:05.832957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/student-stress-monitoring-datasets/StressLevelDataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.834880Z","iopub.execute_input":"2025-08-18T10:50:05.835373Z","iopub.status.idle":"2025-08-18T10:50:05.839677Z","shell.execute_reply.started":"2025-08-18T10:50:05.835341Z","shell.execute_reply":"2025-08-18T10:50:05.838813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.840657Z","iopub.execute_input":"2025-08-18T10:50:05.841135Z","iopub.status.idle":"2025-08-18T10:50:05.855878Z","shell.execute_reply.started":"2025-08-18T10:50:05.841111Z","shell.execute_reply":"2025-08-18T10:50:05.854967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = pd.read_csv(\"/kaggle/input/student-stress-monitoring-datasets/Stress_Dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.857920Z","iopub.execute_input":"2025-08-18T10:50:05.858219Z","iopub.status.idle":"2025-08-18T10:50:05.892958Z","shell.execute_reply.started":"2025-08-18T10:50:05.858196Z","shell.execute_reply":"2025-08-18T10:50:05.892138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.893782Z","iopub.execute_input":"2025-08-18T10:50:05.894107Z","iopub.status.idle":"2025-08-18T10:50:05.929370Z","shell.execute_reply.started":"2025-08-18T10:50:05.894080Z","shell.execute_reply":"2025-08-18T10:50:05.928512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# As they already mentioned that target is Target Variable\n# Which type of stress do you primarily experience?: Eustress, Distress, No Stress\n# So using Stress_Dataset.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.930371Z","iopub.execute_input":"2025-08-18T10:50:05.930681Z","iopub.status.idle":"2025-08-18T10:50:05.934675Z","shell.execute_reply.started":"2025-08-18T10:50:05.930654Z","shell.execute_reply":"2025-08-18T10:50:05.933955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.935451Z","iopub.execute_input":"2025-08-18T10:50:05.935713Z","iopub.status.idle":"2025-08-18T10:50:05.973969Z","shell.execute_reply.started":"2025-08-18T10:50:05.935693Z","shell.execute_reply":"2025-08-18T10:50:05.973105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:05.975115Z","iopub.execute_input":"2025-08-18T10:50:05.975410Z","iopub.status.idle":"2025-08-18T10:50:06.000994Z","shell.execute_reply.started":"2025-08-18T10:50:05.975389Z","shell.execute_reply":"2025-08-18T10:50:06.000149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.001927Z","iopub.execute_input":"2025-08-18T10:50:06.002254Z","iopub.status.idle":"2025-08-18T10:50:06.020659Z","shell.execute_reply.started":"2025-08-18T10:50:06.002227Z","shell.execute_reply":"2025-08-18T10:50:06.019628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.021545Z","iopub.execute_input":"2025-08-18T10:50:06.021793Z","iopub.status.idle":"2025-08-18T10:50:06.042764Z","shell.execute_reply.started":"2025-08-18T10:50:06.021773Z","shell.execute_reply":"2025-08-18T10:50:06.041944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.043685Z","iopub.execute_input":"2025-08-18T10:50:06.043974Z","iopub.status.idle":"2025-08-18T10:50:06.074838Z","shell.execute_reply.started":"2025-08-18T10:50:06.043952Z","shell.execute_reply":"2025-08-18T10:50:06.073973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.075669Z","iopub.execute_input":"2025-08-18T10:50:06.076058Z","iopub.status.idle":"2025-08-18T10:50:06.085991Z","shell.execute_reply.started":"2025-08-18T10:50:06.076030Z","shell.execute_reply":"2025-08-18T10:50:06.084963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df2.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.089859Z","iopub.execute_input":"2025-08-18T10:50:06.090177Z","iopub.status.idle":"2025-08-18T10:50:06.101002Z","shell.execute_reply.started":"2025-08-18T10:50:06.090148Z","shell.execute_reply":"2025-08-18T10:50:06.099955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.102154Z","iopub.execute_input":"2025-08-18T10:50:06.102418Z","iopub.status.idle":"2025-08-18T10:50:06.124711Z","shell.execute_reply.started":"2025-08-18T10:50:06.102399Z","shell.execute_reply":"2025-08-18T10:50:06.123691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.125693Z","iopub.execute_input":"2025-08-18T10:50:06.126109Z","iopub.status.idle":"2025-08-18T10:50:06.145580Z","shell.execute_reply.started":"2025-08-18T10:50:06.126071Z","shell.execute_reply":"2025-08-18T10:50:06.144649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.146543Z","iopub.execute_input":"2025-08-18T10:50:06.146856Z","iopub.status.idle":"2025-08-18T10:50:06.165931Z","shell.execute_reply.started":"2025-08-18T10:50:06.146829Z","shell.execute_reply":"2025-08-18T10:50:06.164855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.166929Z","iopub.execute_input":"2025-08-18T10:50:06.167244Z","iopub.status.idle":"2025-08-18T10:50:06.184502Z","shell.execute_reply.started":"2025-08-18T10:50:06.167216Z","shell.execute_reply":"2025-08-18T10:50:06.183709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Data shape:\", df.shape)\nprint(\"Missing values:\\n\", df.isnull().sum())\nprint(\"Data types:\\n\", df.dtypes)\n\n# Problem type detection based on target variable\ntarget_dtype = df['Which type of stress do you primarily experience?'].dtype\nunique_vals = df['Which type of stress do you primarily experience?'].nunique()\n\nif pd.api.types.is_numeric_dtype(target_dtype):\n    # Further check if discrete or continuous\n    if unique_vals <= 20:  # heuristic threshold for classification\n        problem_type = \"Classification (numeric discrete)\"\n    else:\n        problem_type = \"Regression\"\nelse:\n    problem_type = \"Classification (categorical)\"\n\nprint(f\"Detected problem type: {problem_type}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.185355Z","iopub.execute_input":"2025-08-18T10:50:06.185604Z","iopub.status.idle":"2025-08-18T10:50:06.206507Z","shell.execute_reply.started":"2025-08-18T10:50:06.185583Z","shell.execute_reply":"2025-08-18T10:50:06.205676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Data Types:\\n\", df.dtypes)\nprint(\"\\nMissing Values:\\n\", df.isnull().sum())\nprint(\"\\nUnique Value Counts:\\n\", df.nunique())\n\n# Class distribution & balance detection\nclass_counts = df['Which type of stress do you primarily experience?'].value_counts()\nprint(f\"\\nClass Distribution:\\n{class_counts}\")\n\ntotal = class_counts.sum()\nclass_ratios = class_counts / total\nprint(f\"\\nClass Ratios:\\n{class_ratios}\")\n\n# Identify target variable type\nunique_classes = df['Which type of stress do you primarily experience?'].unique()\nnum_classes = len(unique_classes)\nif num_classes == 2:\n    target_type = \"Binary Classification\"\nelif num_classes > 2:\n    target_type = \"Multiclass Classification\"\nelse:\n    target_type = \"Unknown\"\n\nprint(f\"\\nTarget Variable Type: {target_type}\")\n\n# Auto problem type detection (simplified)\nif pd.api.types.is_numeric_dtype(df['Which type of stress do you primarily experience?'].dtype):\n    if num_classes <= 20:\n        problem_type = \"Classification (numeric discrete)\"\n    else:\n        problem_type = \"Regression\"\nelse:\n    problem_type = \"Classification (categorical)\"\n\nprint(f\"\\nAuto-detected Problem Type: {problem_type}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.207560Z","iopub.execute_input":"2025-08-18T10:50:06.207868Z","iopub.status.idle":"2025-08-18T10:50:06.240200Z","shell.execute_reply.started":"2025-08-18T10:50:06.207839Z","shell.execute_reply":"2025-08-18T10:50:06.239166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rename the column for convenience\ndf = df.rename(columns={'Which type of stress do you primarily experience?': 'stress_type'})\n\n# To permanently rename it in the DataFrame:\n# df.rename(columns={'Which type of stress do you primarily experience?': 'stress_type'}, inplace=True)\n\n# Print the columns to confirm the change\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.241161Z","iopub.execute_input":"2025-08-18T10:50:06.241434Z","iopub.status.idle":"2025-08-18T10:50:06.255102Z","shell.execute_reply.started":"2025-08-18T10:50:06.241412Z","shell.execute_reply":"2025-08-18T10:50:06.254238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Class distribution visualization\nplt.figure(figsize=(8,6))\nsns.countplot(x=df['stress_type'])\nplt.title('Class Distribution of Stress Types')\nplt.ylabel('Count')\nplt.xlabel('Stress Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Numeric features boxplots grouped by class\nnumeric_features = df.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df.select_dtypes(include=['object', 'category']).columns\n\nfor feature in numeric_features:\n    plt.figure(figsize=(6,4))\n    sns.boxplot(x=df['stress_type'], y=df[feature])\n    plt.title(f'Boxplot of {feature} by Class')\n    plt.show()\n\n# Correlation heatmap for numeric features\nplt.figure(figsize=(10,8))\ncorr = df[numeric_features].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Feature Correlation Heatmap')\nplt.show()\n\n# Scatterplot example (if at least two numeric features)\nif len(numeric_features) >= 2:\n    plt.figure(figsize=(10,6))\n    sns.scatterplot(x=df[numeric_features[0]], y=df[numeric_features[1]], hue=df['stress_type'])\n    plt.title(f'Scatterplot of {numeric_features[0]} vs {numeric_features[1]}')\n    plt.show()\n\n\n# Countplots for categorical features\nfor col in cat_cols:\n    plt.figure(figsize=(8, 4))\n    sns.countplot(data=df, x=col, order=df[col].value_counts().index)\n    plt.title(f\"Category Counts for {col}\")\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:06.255995Z","iopub.execute_input":"2025-08-18T10:50:06.256253Z","iopub.status.idle":"2025-08-18T10:50:13.927023Z","shell.execute_reply.started":"2025-08-18T10:50:06.256233Z","shell.execute_reply":"2025-08-18T10:50:13.925983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming 'df' is your DataFrame, and it contains all the features\n# Example: df = pd.read_csv('your_dataset_name.csv')\n\n# --- Step 1: Select only the numerical columns ---\nnumerical_cols = df2.select_dtypes(include=np.number).columns\nnum_cols = df2.select_dtypes(include=np.number).columns\n\n# --- Step 2: Calculate the correlation matrix ---\ncorrelation_matrix = df[numerical_cols].corr()\n\n# --- Step 3: Create the improved heatmap visualization ---\nplt.figure(figsize=(16, 14)) # Increase the figure size for better readability\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n\n# Adjust the title and labels\nplt.title('Feature Correlation Heatmap', fontsize=20)\nplt.xticks(rotation=45, ha='right', fontsize=10)\nplt.yticks(fontsize=10)\n\nplt.tight_layout() # Automatically adjusts plot parameters for a tight layout\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:13.927957Z","iopub.execute_input":"2025-08-18T10:50:13.928213Z","iopub.status.idle":"2025-08-18T10:50:15.941075Z","shell.execute_reply.started":"2025-08-18T10:50:13.928189Z","shell.execute_reply":"2025-08-18T10:50:15.939941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming 'df' is your DataFrame, loaded and clean\n# Example: df = pd.read_csv('your_dataset_name.csv')\n\n# --- Step 1: Select only numerical columns ---\nnumerical_cols = df2.select_dtypes(include=np.number).columns\n\n# --- Step 2: Calculate the correlation matrix ---\ncorrelation_matrix = df2[numerical_cols].corr().abs() # Using .abs() to get absolute values\n\n# --- Step 3: Unstack the matrix to get a list of pairs ---\n# This converts the matrix into a Series with a MultiIndex\ncorr_pairs = correlation_matrix.unstack()\n\n# --- Step 4: Sort the pairs by correlation value ---\nsorted_pairs = corr_pairs.sort_values(kind=\"quicksort\", ascending=False)\n\n# --- Step 5: Filter for high correlations (e.g., above 0.8) ---\nhigh_corr_pairs = sorted_pairs[(sorted_pairs > 0.8) & (sorted_pairs < 1.0)]\n\nprint(\"--- Highly Correlated Feature Pairs ---\")\nprint(high_corr_pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:15.942115Z","iopub.execute_input":"2025-08-18T10:50:15.942382Z","iopub.status.idle":"2025-08-18T10:50:15.980721Z","shell.execute_reply.started":"2025-08-18T10:50:15.942361Z","shell.execute_reply":"2025-08-18T10:50:15.979839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histograms for numerical features\ndf[num_cols].hist(bins=20, figsize=(15, 10))\nplt.suptitle(\"Numerical Feature Distributions\")\nplt.show()\n\n# Boxplots for outlier visualization\nfor col in num_cols:\n    plt.figure(figsize=(6, 3))\n    sns.boxplot(x=df[col])\n    plt.title(f\"Boxplot of {col}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:15.981616Z","iopub.execute_input":"2025-08-18T10:50:15.981964Z","iopub.status.idle":"2025-08-18T10:50:22.427604Z","shell.execute_reply.started":"2025-08-18T10:50:15.981926Z","shell.execute_reply":"2025-08-18T10:50:22.426800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histograms for numerical features\ndf[num_cols].hist(bins=20, figsize=(40, 30))\nplt.suptitle(\"Numerical Feature Distributions\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:22.428584Z","iopub.execute_input":"2025-08-18T10:50:22.428992Z","iopub.status.idle":"2025-08-18T10:50:27.125923Z","shell.execute_reply.started":"2025-08-18T10:50:22.428959Z","shell.execute_reply":"2025-08-18T10:50:27.124885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import shapiro\n\n# Assuming 'df' is your DataFrame, already loaded.\n# Correctly get a list of the numerical column names\ncolumns_to_check = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Create an empty list to store the results\nresults = []\n\nfor col in columns_to_check:\n    # Drop missing values for the statistical test\n    data_to_test = df[col].dropna()\n\n    # Perform the Shapiro-Wilk test\n    if len(data_to_test) >= 3:\n        stat, p_value = shapiro(data_to_test)\n    else:\n        stat, p_value = np.nan, np.nan\n\n    # Determine the conclusion\n    if p_value > 0.05:\n        conclusion = \"Normal\"\n    else:\n        conclusion = \"Not Normal\"\n\n    # Append the results to the list\n    results.append({\n        'Column': col,\n        'Shapiro-Wilk Statistic': f\"{stat:.4f}\",\n        'P-value': f\"{p_value:.4f}\",\n        'Conclusion': conclusion\n    })\n\n# Convert the list of dictionaries to a DataFrame for neat display\nnormality_results_df = pd.DataFrame(results)\n\n# Display the summary table\nprint(normality_results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:27.127052Z","iopub.execute_input":"2025-08-18T10:50:27.127360Z","iopub.status.idle":"2025-08-18T10:50:27.149763Z","shell.execute_reply.started":"2025-08-18T10:50:27.127337Z","shell.execute_reply":"2025-08-18T10:50:27.148964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndef plot_boxplots(data):\n    \"\"\"\n    Plot boxplots for all numeric columns in a DataFrame.\n    \"\"\"\n    # Find all numeric columns in the DataFrame\n    numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n\n    # Check if any numeric columns were found\n    if not numeric_cols:\n        print(\"No numeric columns found to plot.\")\n        return\n\n    # Loop through the list of numeric column names\n    for col in numeric_cols:\n        plt.figure(figsize=(8, 4)) # Adjusted figure size for better readability\n        sns.boxplot(x=data[col])\n        plt.title(f\"Boxplot of {col}\")\n        plt.show()\n\nplot_boxplots(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:27.150733Z","iopub.execute_input":"2025-08-18T10:50:27.151059Z","iopub.status.idle":"2025-08-18T10:50:30.499946Z","shell.execute_reply.started":"2025-08-18T10:50:27.151033Z","shell.execute_reply":"2025-08-18T10:50:30.499154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import shapiro\n\n# Assuming 'df' is your DataFrame, already loaded.\n# Replace this line with your actual data loading code if needed.\n# df = pd.read_csv('your_dataset_name.csv')\n\n# Get a list of the numerical column names\ncolumns_to_check = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Create a list to store the results\nnormality_results = []\n\nfor col in columns_to_check:\n    # Perform the Shapiro-Wilk test on the column data\n    data_to_test = df[col].dropna()\n    \n    # The test requires at least 3 data points\n    if len(data_to_test) >= 3:\n        stat, p_value = shapiro(data_to_test)\n        \n        # Determine if the column is \"Normal\" or \"Not Normal\" based on the p-value\n        if p_value > 0.05:\n            conclusion = \"Normal\"\n        else:\n            conclusion = \"Not Normal\"\n    else:\n        stat, p_value, conclusion = np.nan, np.nan, \"Not enough data\"\n    \n    # Add the results to our list\n    normality_results.append({\n        'Column': col,\n        'Shapiro-Wilk Statistic': f'{stat:.4f}',\n        'P-value': f'{p_value:.4f}',\n        'Conclusion': conclusion\n    })\n\n# Convert the list of results into a pandas DataFrame for a clean table\nresults_df = pd.DataFrame(normality_results)\n\n# Display the summary table\nprint(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.501037Z","iopub.execute_input":"2025-08-18T10:50:30.501354Z","iopub.status.idle":"2025-08-18T10:50:30.521832Z","shell.execute_reply.started":"2025-08-18T10:50:30.501326Z","shell.execute_reply":"2025-08-18T10:50:30.520890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import shapiro\n\n# Assuming 'df' is your DataFrame, already loaded.\n# For example: df = pd.read_csv('your_dataset_name.csv')\n\n# --- Define the IQR Outlier Detection Function ---\ndef has_outliers_iqr(data, col):\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n    return 'Yes' if not outliers.empty else 'No'\n\n# --- Get a list of all numerical column names ---\nnumerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Create a list to store the summary results\nsummary_results = []\n\nfor col in numerical_cols:\n    # 1. Check for Normality\n    data_to_test = df[col].dropna()\n    if len(data_to_test) >= 3:\n        stat, p_value = shapiro(data_to_test)\n        is_normal = 'Normal' if p_value > 0.05 else 'Not Normal'\n    else:\n        is_normal = 'Not enough data'\n\n    # 2. Check for Outliers\n    has_outliers = has_outliers_iqr(df, col)\n\n    summary_results.append({\n        'Column': col,\n        'Normality': is_normal,\n        'Has Outliers (IQR)': has_outliers\n    })\n\n# Create a clean DataFrame from the summary list\nsummary_df = pd.DataFrame(summary_results)\n\n# Display the summary table\nprint(summary_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.522774Z","iopub.execute_input":"2025-08-18T10:50:30.523172Z","iopub.status.idle":"2025-08-18T10:50:30.589602Z","shell.execute_reply.started":"2025-08-18T10:50:30.523143Z","shell.execute_reply":"2025-08-18T10:50:30.588668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import shapiro\n\n# Assuming 'df' is your DataFrame, already loaded.\n# For example: df = pd.read_csv('your_dataset_name.csv')\n\n# --- Step 1: Functions to Identify and Treat Outliers ---\n\ndef has_outliers_iqr(data, col):\n    \"\"\"Checks if a column has outliers using the IQR method.\"\"\"\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n    return 'Yes' if not outliers.empty else 'No'\n\ndef winsorize_column(data, col):\n    \"\"\"Caps outliers in a column using the IQR method bounds.\"\"\"\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])\n    data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])\n    \n    print(f\"Outliers in '{col}' have been capped.\")\n\n\n# --- Step 2: Get a Summary of All Numerical Columns ---\n\nnumerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\nsummary_results = []\n\nfor col in numerical_cols:\n    has_outliers = has_outliers_iqr(df, col)\n    summary_results.append({\n        'Column': col,\n        'Has Outliers (IQR)': has_outliers\n    })\n\nsummary_df = pd.DataFrame(summary_results)\n\nprint(\"--- Outlier Summary Before Winsorization ---\")\nprint(summary_df)\n\n\n# --- Step 3: Automatically Identify and Winsorize Columns ---\n\n# Filter the summary table to find only the columns with outliers\ncolumns_to_winsorize = summary_df[summary_df['Has Outliers (IQR)'] == 'Yes']['Column'].tolist()\n\nprint(\"\\n--- Starting Winsorization ---\")\nif not columns_to_winsorize:\n    print(\"No columns to winsorize.\")\nelse:\n    for col in columns_to_winsorize:\n        winsorize_column(df, col)\n\nprint(\"\\nWinsorization complete for all identified columns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.590497Z","iopub.execute_input":"2025-08-18T10:50:30.590788Z","iopub.status.idle":"2025-08-18T10:50:30.656384Z","shell.execute_reply.started":"2025-08-18T10:50:30.590759Z","shell.execute_reply":"2025-08-18T10:50:30.655493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#As all the num cols are not noral. so better go with median\nfrom sklearn.impute import SimpleImputer\n\nnumerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n\n# import pandas as pd\nimport numpy as np\n\n\n\n# # --- Impute Numerical Columns with the Median ---\n# numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n# for col in numerical_cols:\n#     median_val = df[col].median()\n#     df[col].fillna(median_val, inplace=True)\n#     print(f\"Imputed missing values in '{col}' with the median ({median_val}).\")\n\n\n# # --- Impute Categorical Columns with the Mode ---\n# categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n# for col in categorical_cols:\n#     mode_val = df[col].mode()[0] # .mode() returns a Series, so we take the first value\n#     df[col].fillna(mode_val, inplace=True)\n#     print(f\"Imputed missing values in '{col}' with the mode ({mode_val}).\")\n\n# print(\"\\nImputation complete.\")\n\n\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nimport numpy as np\n\n# Assuming 'df' is your DataFrame with missing values\n# Example: df = pd.read_csv('your_dataset_name.csv')\n\n# --- 1. Identify Numerical and Categorical Columns ---\nnum_cols = df.select_dtypes(include=np.number).columns\ncat_cols = df.select_dtypes(include=['object']).columns\n\n# --- 2. Create Imputer Objects ---\n# Use the median for numerical columns (best for non-normal data)\nmedian_imputer = SimpleImputer(strategy='median')\n\n# Use the most frequent value for categorical columns\nmode_imputer = SimpleImputer(strategy='most_frequent')\n\n# --- 3. Impute Missing Values ---\n# Impute numerical columns\ndf[num_cols] = median_imputer.fit_transform(df[num_cols])\n\n# Impute categorical columns\ndf[cat_cols] = mode_imputer.fit_transform(df[cat_cols])\n\nprint(\"Imputation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.657321Z","iopub.execute_input":"2025-08-18T10:50:30.657598Z","iopub.status.idle":"2025-08-18T10:50:30.813723Z","shell.execute_reply.started":"2025-08-18T10:50:30.657565Z","shell.execute_reply":"2025-08-18T10:50:30.812916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.814604Z","iopub.execute_input":"2025-08-18T10:50:30.814816Z","iopub.status.idle":"2025-08-18T10:50:30.828417Z","shell.execute_reply.started":"2025-08-18T10:50:30.814799Z","shell.execute_reply":"2025-08-18T10:50:30.827617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Assuming 'df' is your DataFrame, loaded and with outliers treated\n# Example: df = pd.read_csv('your_dataset_name.csv')\n\n# --- Step 1: Encode the Target Variable ---\n# The target variable is 'stress_type'\ntarget_col = 'stress_type'\nencoder = LabelEncoder()\ndf[target_col] = encoder.fit_transform(df[target_col])\nprint(\"Target variable encoded.\")\n\n# --- Step 2: Scale the Numerical Features ---\n# All columns except the target are features\n# All cols are not normal so selecting minmax btw 0 & 1\nfeatures = df.drop(columns=[target_col]).columns\nscaler = MinMaxScaler()\ndf[features] = scaler.fit_transform(df[features])\nprint(\"Features scaled using Min-Max Scaler.\")\n\n# --- Step 3: Split the Data ---\nX = df.drop(columns=[target_col])\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"\\nData splitting complete.\")\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.829331Z","iopub.execute_input":"2025-08-18T10:50:30.830301Z","iopub.status.idle":"2025-08-18T10:50:30.862551Z","shell.execute_reply.started":"2025-08-18T10:50:30.830277Z","shell.execute_reply":"2025-08-18T10:50:30.861727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.feature_selection import RFE\n# from sklearn.linear_model import LogisticRegression\n# import pandas as pd\n\n\n# model = LogisticRegression()\n# rfe = RFE(model, n_features_to_select=5)\n# fit = rfe.fit(X, y)\n# print(\"Selected features:\", X.columns[fit.support_].tolist())\n\n\nimport pandas as pd\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n\n# --- Step 1: Run RFE to get the selected features ---\nmodel = LogisticRegression(solver='liblinear') # Added solver for a common warning\nrfe = RFE(model, n_features_to_select=5)\nfit = rfe.fit(X, y)\n\n# --- Step 2: Automatically select the columns and create a new DataFrame ---\n# 'fit.support_' returns a boolean array (True for selected, False for not)\nselected_features_mask = fit.support_\n\n# Use the boolean array to get the names of the selected columns\nselected_features_names = X.columns[selected_features_mask].tolist()\n\n# Create a new DataFrame with only the selected features\nX_selected = X[selected_features_names]\n\nprint(\"Selected features:\", X_selected.columns.tolist())\nprint(\"New DataFrame created with shape:\", X_selected.shape)\n\n# --- Step 3: Perform the train-test split using the new DataFrame ---\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\nprint(\"\\nData splitting complete using the new feature set.\")\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:30.863531Z","iopub.execute_input":"2025-08-18T10:50:30.864024Z","iopub.status.idle":"2025-08-18T10:50:31.024365Z","shell.execute_reply.started":"2025-08-18T10:50:30.864001Z","shell.execute_reply":"2025-08-18T10:50:31.023367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_selected.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:31.025381Z","iopub.execute_input":"2025-08-18T10:50:31.025633Z","iopub.status.idle":"2025-08-18T10:50:31.038078Z","shell.execute_reply.started":"2025-08-18T10:50:31.025612Z","shell.execute_reply":"2025-08-18T10:50:31.037104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.feature_selection import SelectKBest, chi2\n# import pandas as pd\n\n\n# # Select top 1 feature based on chi2 test\n# selector = SelectKBest(score_func=chi2, k=1)\n# X_new = selector.fit_transform(X, y)\n# print(\"Selected feature shape:\", X_new.shape)\n\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\n# Select the top 5 features based on the chi-squared test\nselector = SelectKBest(score_func=chi2, k=5)\nselector.fit(X, y)\n\n# Get a boolean mask of the selected features\nselected_features_mask = selector.get_support()\n\n# Use the mask to get the names of the selected columns\nselected_features_names = X.columns[selected_features_mask].tolist()\n\nprint(\"Selected features by Chi-Squared:\", selected_features_names)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:31.039048Z","iopub.execute_input":"2025-08-18T10:50:31.039400Z","iopub.status.idle":"2025-08-18T10:50:31.067881Z","shell.execute_reply.started":"2025-08-18T10:50:31.039370Z","shell.execute_reply":"2025-08-18T10:50:31.066930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TO find Imbalance data before modelling\n\nprint(\"Class distribution of the target variable:\")\nprint(df['stress_type'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:31.068662Z","iopub.execute_input":"2025-08-18T10:50:31.068951Z","iopub.status.idle":"2025-08-18T10:50:31.077071Z","shell.execute_reply.started":"2025-08-18T10:50:31.068931Z","shell.execute_reply":"2025-08-18T10:50:31.076058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Selected features:\", X_selected.columns.tolist())\nprint(\"New DataFrame created with shape:\", X_selected.shape)\n\nX = X_selected\ny = df['stress_type']\n\n# --- Step 3: Perform the train-test split using the new DataFrame ---\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\nprint(\"\\nData splitting complete using the new feature set.\")\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:50:31.078179Z","iopub.execute_input":"2025-08-18T10:50:31.078467Z","iopub.status.idle":"2025-08-18T10:50:31.101159Z","shell.execute_reply.started":"2025-08-18T10:50:31.078438Z","shell.execute_reply":"2025-08-18T10:50:31.100111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install imbalanced-learn\n# First, completely uninstall both packages\n!pip uninstall -y scikit-learn imbalanced-learn\n\n# Now, install known stable versions\n!pip install scikit-learn==1.0.2 imbalanced-learn==0.9.1\n\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, y)\nprint(\"Original dataset shape:\", y.value_counts())\nprint(\"Resampled dataset shape:\", pd.Series(y_res).value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:52:13.669684Z","iopub.execute_input":"2025-08-18T10:52:13.670072Z","iopub.status.idle":"2025-08-18T10:53:40.459025Z","shell.execute_reply.started":"2025-08-18T10:52:13.670030Z","shell.execute_reply":"2025-08-18T10:53:40.457884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Assuming you have the following variables from your previous steps:\n# X_train_res, y_train_res (your balanced training data)\n# X_test, y_test (your original, unbalanced testing data)\n\n# --- Step 1: Initialize and train the model ---\n# Use the balanced data to train the model\nmodel = LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000)\nmodel.fit(X_res, y_res)\n\n# --- Step 2: Make predictions on the original test set ---\n# The model has learned from a balanced set, but it should be tested on real data\ny_pred = model.predict(X_test)\n\n# --- Step 3: Evaluate the model ---\nprint(\"--- Classification Report ---\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\n--- Confusion Matrix ---\")\nprint(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:09:41.803561Z","iopub.execute_input":"2025-08-18T11:09:41.803955Z","iopub.status.idle":"2025-08-18T11:09:41.829419Z","shell.execute_reply.started":"2025-08-18T11:09:41.803926Z","shell.execute_reply":"2025-08-18T11:09:41.828561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score\nimport warnings\n\nwarnings.filterwarnings('ignore') # This ignores potential warnings from the models\n\n# Assuming you have the following variables from your previous steps:\n# X_train_res, y_train_res (your balanced training data)\n# X_test, y_test (your original testing data)\n\n# --- Step 1: Define the models you want to test ---\nmodels = {\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'SVC': SVC(random_state=42),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n}\n\n# --- Step 2: Loop through each model and build a pipeline ---\nfor name, model in models.items():\n    # Since you already scaled your data, we don't need the scaler in the pipeline.\n    # We will just use the classifier directly.\n    pipeline = Pipeline(steps=[\n        ('classifier', model)\n    ])\n\n    # Fit the pipeline on the BALANCED training data\n    pipeline.fit(X_res, y_res)\n\n    # Make predictions on the ORIGINAL test data\n    y_pred = pipeline.predict(X_test)\n\n    # --- Step 3: Calculate and print accuracy and precision scores ---\n    accuracy = accuracy_score(y_test, y_pred)\n    # The macro average is the average precision for all classes\n    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n    \n    print(f\"--- Results for {name} ---\")\n    print(f\"Accuracy Score: {accuracy:.2f}\")\n    print(f\"Macro Precision Score: {precision:.2f}\")\n    print(\"\\n--- Full Classification Report ---\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n    print(\"--------------------------------------------------\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:35:42.510457Z","iopub.execute_input":"2025-08-18T11:35:42.510798Z","iopub.status.idle":"2025-08-18T11:35:44.154381Z","shell.execute_reply.started":"2025-08-18T11:35:42.510770Z","shell.execute_reply":"2025-08-18T11:35:44.153546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.preprocessing import StandardScaler\n\n# # Assuming you have your final balanced data and test data ready from previous steps.\n# # X_res, y_res, X_test, y_test\n\n# # --- Step 1: Initialize and train the final model ---\n# model = DecisionTreeClassifier(random_state=42)\n# model.fit(X_res, y_res)\n\n# # --- Step 2: Select a single new data point from your main DataFrame ---\n# # Automatically get the feature names from your balanced training data.\n# features = X_res.columns.tolist()\n\n# # Select a single row for prediction.\n# # Change the index number [100] to select any row you want from your original DataFrame.\n# single_data_point = df.iloc[[100]] \n\n# new_data_features = single_data_point[features]\n\n# # Extract the actual stress_type to verify the prediction.\n# # Make sure to replace 'stress_type' if your target column has a different name.\n# actual_stress_type = single_data_point['stress_type'].values[0]\n\n# # --- Step 3: Scale the new data point ---\n# scaler = StandardScaler()\n# scaler.fit(X_res)\n# scaled_new_data = scaler.transform(new_data_features)\n\n# # --- Step 4: Make the prediction ---\n# predicted_stress_type = model.predict(scaled_new_data)\n# predicted_class = predicted_stress_type[0]\n\n# # --- Step 5: Print and verify the result ---\n# print(\"\\nPrediction Results:\")\n# print(f\"The model predicted the stress type is: {predicted_class}\")\n# print(f\"The actual stress type is: {actual_stress_type}\")\n\n# # Cross-verify\n# if predicted_class == actual_stress_type:\n#     print(\"The prediction is correct! ✅\")\n# else:\n#     print(\"The prediction is incorrect. ❌\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:40:33.041270Z","iopub.execute_input":"2025-08-18T11:40:33.041605Z","iopub.status.idle":"2025-08-18T11:40:33.065303Z","shell.execute_reply.started":"2025-08-18T11:40:33.041575Z","shell.execute_reply":"2025-08-18T11:40:33.063676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\n\n# --- Step 1: Initialize and train the final model ---\n# We will use the Decision Tree Classifier as it was the best performer.\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_res, y_res) # Using your balanced and scaled training data\n\n# --- Step 2: Select 5 new data points from your main DataFrame ---\n# Change the range [100:105] to select a different batch of rows.\ntest_records = df.iloc[100:120]\n\n# Get the list of features from your training data to ensure consistency.\nfeatures = X_res.columns.tolist()\nnew_data_features = test_records[features]\n\n# Get the actual stress_type values for verification.\nactual_stress_types = test_records['stress_type'].values\n\n# --- Step 3: Make the predictions on the already scaled data ---\n# We are skipping the scaling step here because your data is already scaled\n# from your previous processing steps.\npredicted_stress_types = model.predict(new_data_features)\n\n# --- Step 4: Compare and print the results ---\nresults_df = pd.DataFrame({\n    'Record Index': test_records.index,\n    'Actual Stress Type': actual_stress_types,\n    'Predicted Stress Type': predicted_stress_types\n})\n\nresults_df['Correct'] = results_df['Actual Stress Type'] == results_df['Predicted Stress Type']\n\n\nprint(\"Prediction Results for 5 Records:\")\nprint(results_df)\n\ncorrect_predictions = results_df['Correct'].sum()\nprint(f\"\\nCorrect predictions: {correct_predictions} out of 5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:50:00.082558Z","iopub.execute_input":"2025-08-18T11:50:00.082865Z","iopub.status.idle":"2025-08-18T11:50:00.105923Z","shell.execute_reply.started":"2025-08-18T11:50:00.082844Z","shell.execute_reply":"2025-08-18T11:50:00.104975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}